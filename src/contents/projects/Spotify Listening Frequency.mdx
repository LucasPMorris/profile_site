---
id: 1
title: 'Spotify Usage Heatmap'
stacks: ['JavaScript', 'TypeScript', 'Node.js', 'React', 'Next.js', 'PostgreSQL', 'Prisma', 'TailwindCSS']
slug: 'spotify-usage-heatmap'
image: 'https://lucas.untethered4life.com//images/projects/spotify/spotify-heatmap-banner.png'
link_demo: 'https://lucas.untethered4life.com/dashboard'
link_github: 'https://lucas.untethered4life.com/dashboard'
created_at: '2025-09-25'
updated_at: '2025-09-25'
---

# Spotify Usage Heatmap

Many developers have likely seen the classic Github repo submit history map. A visualization where each square in A GRID represents a day. The color intensity of each square indicating the number of commits made on that day. Darker squares represent days with more commits, while lighter squares represent days with fewer commits. A visualization that helps to quickly review coding activity over time.

After including a Spotify "Now Playing" component on this site, which was pulled from a Github repository, my curiosity got the best of me and I delved into a rabbit hole of REST API calls, relational database sctructures, data transormations, jq bash scripts, server side cron jobs, and a whole lot more. The end result was a dashboard that visualizes my own music listening habits over time, similar to that well known Github commit history map most developers today know.

In this article I will describe:
- The steps I took to build the dashboard.
- The challenges I faced along the way.
- The technologies I used to bring it all together.
- And the overall lessons and insights I gained from the experience.

---

# Getting Started
<span id="getting-started" name="Getting Started" data-toc></span>

## Sampling Spotify Data

I started by sampling data returned from the Spotify API. I used [Insomnia](https://insomnia.rest/) to make REST API calls to Spotify and view the returned data. I find that Insomnia has a cleaner interface than Postman and is easier to use for simple API calls, I know, probably an unpopular opinion. Anyway...to get started you have to:

- Create a [Spotify Developer account](https://developer.spotify.com/) and register an application. 
- Obtain necessary token to make API calls. I don't cover that here, the steps for obtaining a Spotify auth token can be found [HERE](https://developer.spotify.com/documentation/web-api/tutorials/code-flow).
- Call the Spotify `v1/me/player/recently-played` API using Insomnia.

<br />

```
##Request
#canCollapse
curl --request GET \
  --url 'https://api.spotify.com/v1/me/player/recently-played?limit=50' \
  --header 'Authorization: Bearer token' \
  --header 'Content-Type: application/json'
```

<br />

```
##Response
#canCollapse
#language-json
{
"played_at": "2025-08-29T00:00:27.043Z",  
"context": {
	"type": "playlist",
	"external_urls": {
	  "spotify": "https://open.spotify.com/playlist/4gowSSvFaSz8Auzizrjkyw"
	},
	"href": "https://api.spotify.com/v1/playlists/4gowSSvFaSz8Auzizrjkyw",
	"uri": "spotify:playlist:4gowSSvFaSz8Auzizrjkyw"
},
"track": {		
	"album": {
	  "album_type": "album",
	  "artists": [
		{
		  "external_urls": {
			"spotify": "https://open.spotify.com/artist/378dH6EszOLFShpRzAQkVM"
		  },
		  "href": "https://api.spotify.com/v1/artists/378dH6EszOLFShpRzAQkVM",
		  "id": "378dH6EszOLFShpRzAQkVM",
		  "name": "Lindsey Stirling",
		  "type": "artist",
		  "uri": "spotify:artist:378dH6EszOLFShpRzAQkVM"
		}
	  ],
	  "available_markets": [ "AR", "AU", "..." ],
	  "external_urls": {
		"spotify": "https://open.spotify.com/album/5EH0A5mhsGNCOPPpvi3RfF"
	  },
	  "href": "https://api.spotify.com/v1/albums/5EH0A5mhsGNCOPPpvi3RfF",
	  "id": "5EH0A5mhsGNCOPPpvi3RfF",
	  "images": [
		{
		  "url": "https://i.scdn.co/image/ab67616d0000b273af2acf076c7856bfe6ef580e",
		  "width": 640,
		  "height": 640
		},
		{
		  "url": "https://i.scdn.co/image/ab67616d00001e02af2acf076c7856bfe6ef580e",
		  "width": 300,
		  "height": 300
		},
		{
		  "url": "https://i.scdn.co/image/ab67616d00004851af2acf076c7856bfe6ef580e",
		  "width": 64,
		  "height": 64
		}
	  ],
	  "name": "Brave Enough",
	  "release_date": "2016-08-19",
	  "release_date_precision": "day",
	  "total_tracks": 14,
	  "type": "album",
	  "uri": "spotify:album:5EH0A5mhsGNCOPPpvi3RfF"
	},
	"artists": [
	  {
		"external_urls": {
		  "spotify": "https://open.spotify.com/artist/378dH6EszOLFShpRzAQkVM"
		},
		"href": "https://api.spotify.com/v1/artists/378dH6EszOLFShpRzAQkVM",
		"id": "378dH6EszOLFShpRzAQkVM",
		"name": "Lindsey Stirling",
		"type": "artist",
		"uri": "spotify:artist:378dH6EszOLFShpRzAQkVM"
	  },
	  {
		"external_urls": {
		  "spotify": "https://open.spotify.com/artist/31rVRoX5ZG9ZyRbHvlEwjA"
		},
		"href": "https://api.spotify.com/v1/artists/31rVRoX5ZG9ZyRbHvlEwjA",
		"id": "31rVRoX5ZG9ZyRbHvlEwjA",
		"name": "RuthAnne",
		"type": "artist",
		"uri": "spotify:artist:31rVRoX5ZG9ZyRbHvlEwjA"
	  }
	],
	"available_markets": [ "AR", "AU", "..." ],
	"disc_number": 1,
	"duration_ms": 229120,
	"explicit": false,
	"external_ids": {
	  "isrc": "TCACO1625907"
	},
	"external_urls": {
	  "spotify": "https://open.spotify.com/track/0uFwdpkVJzpyUxlu9reTWK"
	},
	"href": "https://api.spotify.com/v1/tracks/0uFwdpkVJzpyUxlu9reTWK",
	"id": "0uFwdpkVJzpyUxlu9reTWK",
	"is_local": false,
	"name": "Love's Just a Feeling (feat. Rooty)",
	"popularity": 39,
	"preview_url": null,
	"track_number": 12,
	"type": "track",
	"uri": "spotify:track:0uFwdpkVJzpyUxlu9reTWK"
  }
}
```

---

## Initial Database Setup

This allowed me to jump into the design of the database schema. Keeping it simple initially, I started with unique tables for tracks, artists, albums, and play history. I stripped away any fields I didn't want to use. 
- `sptrack` - Table to store details about each Spotify track that I have listened to.
- `spartist` - Table to store details about each Spotify artist that I have listened to.
- `spalbum` - Table to store details about each Spotify album that I have listened to.
- `spplayhistory` - Table to store details about my Spotify play history.

I included some very simple relationships defined between the tables. While this was a good start, I knew that I would need to iterate on this design a great deal before it could would be a performant and efficient way to store and retrieve my listening history.

<div className="flex flex-col lg:flex-row gap-5">  

  <img src="/images/projects/spotify/sptrack-block-first.svg" width="250" alt="Table Diagram - sptrack" />
  <img src="/images/projects/spotify/spartist-block-first.svg" width="250" alt="Table Diagram - spartist" />
  <img src="/images/projects/spotify/spalbum-block-first.svg" width="250" alt="Table Diagram - spalbum" />
  <img src="/images/projects/spotify/spplayhistory-block-end.svg" width="250" alt="Table Diagram - spplayhistory" />

</div>

<div data-component="CallOut" data-header="Important Detail">
  Spotify Play History API is limited to the last 50 tracks played.

  On request Spotify will send you all of your data with a notice that it may take up to 30 days to process.
</div>

I submitted a request for my data and, despite the 30 day notice, I received an email with a link to download my data within a week. The download included 4 JSON files containing roughly 2 years of my Spotify listening history each. 
  - The files were ~47MB and contained just over 60,000 records.
  - I anticipated that data would be in a similar format to the Spotify API, but it was different and missing a lot of details.

```
##Bulk Play History Sample
#canCollapse
{
  "ts": "2017-07-27T00:56:06Z",
  "platform": "Android OS 7.0 API 24 (LGE, LGLS992)",
  "ms_played": 75338,
  "conn_country": "US",
  "ip_addr": "73.181.8.221",
  "master_metadata_track_name": "Love's Just a Feeling (feat. Rooty)",
  "master_metadata_album_artist_name": "Lindsey Stirling",
  "master_metadata_album_album_name": "Brave Enough",
  "spotify_track_uri": "spotify:track:0uFwdpkVJzpyUxlu9reTWK",
  "episode_name": null,
  "episode_show_name": null,
  "spotify_episode_uri": null,
  "audiobook_title": null,
  "audiobook_uri": null,
  "audiobook_chapter_uri": null,
  "audiobook_chapter_title": null,
  "reason_start": "clickrow",
  "reason_end": "endplay",
  "shuffle": false,
  "skipped": false,
  "offline": false,
  "offline_timestamp": null,
  "incognito_mode": false
}
```

---

# Preparing Historical Data
<span id="data-preperation" name="Data Prep" data-toc></span>

The next series of steps involved running a handful of one off node and bash scripts to a) transform the historical data, b) extract a list of track id's, and c) use those track id's to enrich that data with the missing details.

---

## Transforming the Data

For the transformation, I used array destructuring `items.map(item => {})` and the `...rest` operator to remove unnecessary key value pairs from the bulk data set. The output matched the data form which would be returned from the `/v1/me/player/recently-played` endpoint. I dumped the results out to a new JSON file, `ProcessedSpotifyHistory.json`.

<div data-component="CallOut" data-header="Improvements">

The functions `removeUnwantedPropertiesFromHistory()` and `transformHistoryData(history)` in the code snippet below could be wrapped into one function. I was in a flow state and knew this would only need to run once, so I just kept flowing.

</div>

```
## Data Transformation Script
#canCollapse
// Data files provided by Spotify
import History1 from './History1.json' with { type: 'json' }; 
import History2 from './History2.json' with { type: 'json' };
import History3 from './History3.json' with { type: 'json' };
import History4 from './History4.json' with { type: 'json' };

import fs from 'fs';
import path from 'path';

function removeUnwantedPropertiesFromHistory(history) {
  return history.map(item => {
    const { 
      platform,
      ms_played,
      conn_country,
      ip_addr,
      master_metadata_album_artist_name,
      episode_name, episode_show_name,
      spotify_episode_uri,
      shuffle,
      audiobook_title,
      audiobook_uri,
      audiobook_chapter_uri,
      audiobook_chapter_title,
      reason_start, reason_end,
      skipped,
      offline,
      offline_timestamp,
      incognito_mode,
      ...rest } = item;
      return rest;
  });
}

function transformHistoryData(history) {
  return history.map(item => {
    item.played_at = item.ts;
    item.album = { name: item.master_metadata_album_album_name || 'Unknown Album' };
    item.title = item.master_metadata_track_name;
    item.songUrl = `https://open.spotify.com/track/${item.spotify_track_uri?.split(':').pop()}`;
    delete item.ts;
    delete item.master_metadata_album_album_name;
    delete item.master_metadata_track_name;
    delete item.spotify_track_uri;
    return item;
  });
}

async function processAndSaveHistory() {
  const allHistory = [...History1, ...History2, ...History3, ...History4];
  const cleanedHistory = removeUnwantedPropertiesFromHistory(allHistory);
  const transformedHistory = transformHistoryData(cleanedHistory);

  if (transformedHistory.length === 0) {
    console.warn('‚ö†Ô∏è No records to write. Check input files.');
    return;
  }

  const directory = process.cwd();
  const outputPath = path.join(directory, 'ProcessedSpotifyHistory.json');
  fs.writeFileSync(outputPath, JSON.stringify(transformedHistory, null, 2));
  console.log(`Processed history saved to ${outputPath}`);
}

processAndSaveHistory().catch(console.error);
```

```
## Bulk Play History Transformed - ProcessedSpotifyHistory.json
#canCollapse
#language-json
{
  "id": "0uFwdpkVJzpyUxlu9reTWK",
  "played_at": "2017-07-27T00:57:32Z",
  "album": {
    "name": "Brave Enough"
  },
  "title": "Love's Just a Feeling (feat. Rooty)",
  "songUrl": "https://open.spotify.com/track/0uFwdpkVJzpyUxlu9reTWK"
}
```

<div data-component="CallOut" data-header="Important Observations">

- A single track can only be on a single album. However, a single track or a single album can have multiple artists.
- The bulk data did not include an id for the track played, it was embedded in the `spotify_track_uri` however.
- The track_id was also used to compose the the `"songUrl"` key's value.

</div>

Before I could load the data into the database I had to acquire the missing track, album, and artist details.

**YAY!** Spotify has `/v1/artists` and `/v1/tracks` endpoints for bulk data retriaval.

**UGGH!** You can only request 50 artist or track details at a time.

---

## Extracting the Track Id's

- Extract each track id from the end of every songUrl in `ProcessedSpotifyHistory.json`.
- Write every set of 50 track id's to a file as a JSON array, `SpotifyTrackIdList${count}.json`.

This resulted in 1224 files, all but the last file containing 50 track id's!

```
## Extraction Script
#canCollapse
import fs from 'fs';
import path from 'path';

var startTime = new Date();

function getTrackIdListFromHistory(history: any[]) { 
  const trackIdSet = new Set();
  const directory = process.cwd();
  var chunkCount = 1;
  
  while (history.length > 0) {
    const chunk = history.splice(0, 50); // Take the first 50 records
    chunk.forEach(item => {
      if (item.songUrl) {
        const trackId = item.songUrl.split('/').pop();
        if (trackId) {
          trackIdSet.add(trackId);
        }
      }
    });

    const outputPath = path.join(directory, `tools/trackHistoryList/spotifyTrackIdList${chunkCount}.json`);
    fs.writeFileSync(outputPath, JSON.stringify(Array.from(trackIdSet), null, 2));
    log(`Track ID list #${chunkCount} saved to ${outputPath}`);
    
    trackIdSet.clear();
    chunkCount++;
  }
  return Array.from(trackIdSet);
} 
```

---

## Enriching the Data

Enriching the data would require that I call the Spotify API over 1200 times! 

- Open each `spotifyTrackIdList${count}.json` file. 
- Make REST API calls to Spotify's `/v1/tracks?${trackIdList}` endpoint to acquire the additional details for every track.
- Write the returned JSON data to file. 

<div data-component="CallOut" data-header="Coding Tips">

- Use `setTimeout` to space out the API calls to avoid hitting rate limits.
- Monitor the token expiration time and refresh it as needed.
- Log progress after each chunk is processed to troubleshoot edge cases.

</div>


```
## Data Enrichment Script
#canCollapse
import fs from 'fs';
import path from 'path';

async function getTrackDetailFromSpotifyTrackIdList(chunkNumber: number, access_token: any) { // STEP 2: Fetch track details from Spotify API
  const inputPath = path.join(directory, `tools/trackHistoryList/spotifyTrackIdList${chunkNumber}.json`);
  
  if (!fs.existsSync(inputPath)) { log(`Input file not found: ${inputPath}`); return; }

  const trackIdList = JSON.parse(fs.readFileSync(inputPath, 'utf-8'));
  if (trackIdList.length === 0) {
    log('‚ö†Ô∏è No track IDs found in the input file.');
    return;
  }

  const chunkSize = 50; // Spotify API allows up to 50 IDs per request
  const trackDetails: any[] = [];

  for (let i = 0; i < trackIdList.length; i += chunkSize) {
    const chunk = trackIdList.slice(i, i + chunkSize);
    const idsParam = chunk.join(','); 

    try {
      const response = await fetch(`https://api.spotify.com/v1/tracks?ids=${idsParam}`, { headers: { Authorization: `Bearer ${access_token}` } });
      
      if (!response.ok) {
        var responseJson = await response.json();
        log(`Error fetching track details: ${response.status} ${response.statusText}`);
        log(`Response: ${responseJson}`);
        continue;
      }
      
      const data = await response.json();
      trackDetails.push(...data.tracks);
    } catch (error) { log(`Fetch error:, ${JSON.stringify(error)}`); }
  }

  const outputPath = path.join(directory, `tools/trackHistoryData/spotifyTrackDetails${chunkNumber}.json`);
  fs.writeFileSync(outputPath, JSON.stringify(trackDetails, null, 2));
  log(`Track details chunk #${chunkNumber} saved to ${outputPath}`);
}

async function getTrackDetailsInChunks() {
 for (let chunkNumber = 1; chunkNumber <= 1224; chunkNumber++) {       
   getTrackDetailFromSpotifyTrackIdList(chunkNumber, access_token);              
   var endtime = new Date();
   if ((endtime.getMilliseconds() - startTime.getMilliseconds()) > 3300000) access_token = getAccessToken();         
   await new Promise(resolve => setTimeout(resolve, 10000));
 }
}

async function joinFiles() {
  var allTrackDetails = [];
  for (let i = 1; i <= 1224; i++) {
    var inputPath = path.join(process.cwd(), `tools/trackHistoryData/spotifyTrackDetails${i}.json`);
    if(!fs.existsSync(inputPath)) { console.log(`File ${inputPath} does not exist, skipping...`); continue; }
    allTrackDetails = allTrackDetails.concat(JSON.parse(fs.readFileSync(inputPath, 'utf-8')));
    console.log(`File #${i} processed, total tracks so far: ${allTrackDetails.length}`);
  }

  var outputPath = path.join(process.cwd(), `tools/trackHistoryData/finalSpotifyTrackDetails.json`);
  fs.writeFileSync(outputPath, JSON.stringify(allTrackDetails, null, 2));
  console.log(`All files joined. Total tracks: ${allTrackDetails.length}`);
}

(async () => {
  await getTrackDetailsInChunks();
  await joinFiles();
  log('All chunks processed and files joined.');
})();
```
---

## Merging the fetched and transformed data

- Using the command line tool, jq, I added details to the `ProcessedSpotifyHistory.json` for every played song.
- Essentially this adds one more nested level of JSON for the track information with the `played_at` key value at the root.
- Which would look like this: `{ "played_at": "2017-07-27T00:56:06Z", "track": {...} }, { "played_at": "2017-07-27T00:57:32Z", "track": {...} }`
- While this, likely unnecessarily, duplicated a lot of data...I knew that it would be in the same format that future calls to the Spotify API would returns.
 
```
## Merge Track Details and Play History
#canCollapse
jq -s '
  (.[1] | map({ (.id): . }) | add) as $trackDetails
  |
  .[0] | map(
    (
      .track_id = (.songUrl | split("/track/")[1] | split("?")[0]) |
      .detail = $trackDetails[.track_id] |
      if .detail != null then
        {
          "played_at": .played_at,
          "track": {
            "id": .track_id,
            "name": .detail.name,
            "external_ids": { "isrc": .detail.external_ids.isrc },
            "duration_ms": .detail.duration_ms,
            "explicit": .detail.explicit,
            "external_urls": {
              "spotify": .detail.external_urls.spotify
            },
            "album": {
              "id": .detail.album.id,
              "name": .detail.album.name,
              "release_date": .detail.album.release_date, 
              "images": (.detail.album.images // null),
              "artists": (
                .detail.album.artists // [] | map({
                  "id": .id,
                  "name": .name,
                  "external_urls": {
                    "spotify": .external_urls.spotify
                  }
                })
              )
            },
            "artists": (
              .detail.artists | map({
                "id": .id,
                "name": .name,
                "external_urls": {
                  "spotify": .external_urls.spotify
                }
              })
            )
          }
        }
      else
        empty
      end
    )
  )
' "ProcessedSpotifyHistory.json" "finalSpotifyTrackDetails.json" > "merged_spotifyTrackDetails.json"

echo "‚úÖ Merged chunk"
```
---

# Database Design

Now that I had prepared all of my historical data and created a process for ingesting it, it was time to fully build out the database schema.

---

## Defining Table Relationships

Design Considerations:
- **A single track can have multiple artists** (features, collaborations)
- **An album can have multiple artists** (compilations, various artists albums)
- **Determine an efficient way to store the data for use in the heatmap.**
- **Not every album had an image url**
- **Tracks may be on different albums with different images.** (or so I thought, see Feature Enhancement section for more detail)

I began by creating many-to-many relationships between tracks and artists, and albums and artists. Those relationships are managed via the `sptrack_artists` and `spalbum_artists` junction tables. The initial scope for this project was to simply track listening frequency. Counting hourly plays every time, client side, would be incredibly inefficient so a table with pre-counted hourly plays made sense. The creation of `spdailyplaystats` served that purpose. An important note about that table is that `hourly_plays` column is a Json array of 24 numbers for each hour of the day. This one table alone is enough to create an overall listening heatmap. Just grab each row in that table for the date range you want and your good to go.

That table was not enough to get a list of the most listened to artists or tracks for the given time frame however. An overall listening heatmap wasn't good enough for me, I wanted to do more, I wanted to include heatmaps for specific artists or tracks also. The scale and complexity to do that became a headscratcher.

Also, added common_album and common_artist tables to store information about the album that should be visually presented when displaying track information.

<div><div data-component="SvgScroll" data-src="/images/projects/spotify/schema-first-relations.svg" data-width="800" data-height="1000" data-initial-scale="1.0" /></div>

---

## Performance and the Algorithm Realization

I was asking AI all kinds of questions to help. It would throw out different code suggestions, table additions, indexing options, blah blah blah. None of them made any sense or really worked but I knew there had to be someway to do this efficiently. It felt obvious that data had to be precounted, but how....and that is when it hit me. Create different size "buckets" with precounted data for every artist and track.  For every year, month, and day, I would have precounted plays for every single artist and every single track I have ever listened to. Then when retreiving data real-time I could grab the biggest buckets first, followed by the next biggest, then only count the buckets for each day on the edges of the date range.

Below is the final DB schema. The `artiststat` and `trackstat` tables represented precounted stats for each artist and track for any given yearly, monthly, or daily bucket. The `yearlybucket`, `monthlybucket`, and `daybucket` tables containing. The `artiststat` and `trackstat` associated with a day bucket then included `hourly_plays.` Ideally, after using the largest possible buckets to determine the top artists and tracks I could then use the power of relational databases to quickly fetch all of the hourly plays for a specific artist or track.   

<div><div data-component="SvgScroll" data-src="/images/projects/spotify/schema-full.svg" data-width="800" data-height="1000" data-initial-scale="1.0" /></div>

---

# Data Aggregation and Polling
<span id="data-polling" name="Data Polling" data-toc></span>

This process took a lot of trial and error. I wiped tables regularly and started over more times than I would like to admit. The database structure described was derived after a significant number of iterations attempting to load this data into the database. I started with defining what my source of truth was then building outward from there.

## The Source of truth

Spotify's recently played endpoint, `v1/me/player/recently-played`, provided all the data that I needed and I had already transformed all of my historical data to match what was returned from that endpoint. That data can be fed directly into the `spplayhistory` table. For new artists, tracks, or albums, I could create new records as needed after storing the data in `spplayhistory`. I also knew that I could only fetch the last 50 played tracks....I often listen to more than 50 songs in a single day. It was critical that I started doing this immediately and not to interfere with it once it was in place. As long as `spplayhistory` was regularly populated, the rest could be iteratively solved.

- `getRecentlyPlayedFromSpotify()` fetches my recent play history from Spotify.
- `ingestSpotifyPlays()` uses the data returned from `getRecentlyPlayedFromSpotify`

For now I will say that I quickly set up a service so that these functions ran every 30 minutes. How I did that will be described soon.

```
##Fetching recently played from Spotify 
#canCollapse
export const getRecentlyPlayedFromSpotify = async (): Promise<RawRecentlyPlayedResponse> => {
  try {
    const { access_token } = await getAccessToken();  
    const lastSync = await prisma.spplayhistory.findFirst({ orderBy: { played_at: 'desc' }, select: { played_at: true } });
    const afterTimestamp = lastSync?.played_at instanceof Date ? lastSync.played_at.getTime() : 0;

    let url = `${RECENTLY_PLAYED_ENDPOINT}&after=${afterTimestamp}`;
    
    const response = await axios.get(url, { headers: { Authorization: `Bearer ${access_token}` } });
  
    if (response.status === 204 || response.status > 400 || Array.isArray(response.data) || !response.data.items) {
      console.warn('Recently played tracks empty or missing, error status:', response.status);
      return { status: response.status, data: [] };
    }

    return { status: 200, data: response.data.items }; // raw Spotify items
  } catch (error) {
    console.error('Error fetching recently played tracks:', error);
    return { status: 500, data: [] };
  }
};
``` 

<br/>

```
##Formatting and ingesting the returned Spotify plays.
#canCollapse
export const ingestSpotifyPlays = async (): Promise<void> => {
  const response = await getRecentlyPlayedFromSpotify();

  if (response.status !== 200 || !response.data) {
    console.error('Failed to fetch recently played tracks from Spotify');
    return;
  }

  const items = response.data;
  console.log(`Processing ${items.length} plays`);

  // Prepare batch operations
  const albumUpserts: any[] = [];
  const trackUpserts: any[] = [];
  const artistUpserts: any[] = [];
  const playHistoryUpserts: any[] = [];
  const trackArtistRelations: any[] = [];
  const albumArtistRelations: any[] = [];

  // Collect all unique entities
  const allArtistIds = new Set<string>();

  for (const item of items as any[]) {
    const { track, played_at } = item;
    const isrc = track.external_ids?.isrc;

    // Collect album data
    albumUpserts.push({
      where: { album_id: track.album.id },
      update: { name: track.album.name, image_url: track.album.images[0]?.url, release_date: new Date(track.album.release_date) },
      create: {  album_id: track.album.id, name: track.album.name, image_url: track.album.images[0]?.url, release_date: new Date(track.album.release_date) }
    });

    // Collect track data
    trackUpserts.push({
      where: { track_id: track.id },
      update: { title: track.name, isrc, album_id: track.album.id, explicit: track.explicit, song_url: track.external_urls.spotify, duration: Math.floor(track.duration_ms / 1000), release_date: new Date(track.album.release_date)  },
      create: { track_id: track.id, title: track.name, isrc, album_id: track.album.id, explicit: track.explicit, song_url: track.external_urls.spotify, duration: Math.floor(track.duration_ms / 1000), release_date: new Date(track.album.release_date) }
    });

    // Collect artist data and relationships
    for (const artist of [...track.artists, ...track.album.artists]) {
      allArtistIds.add(artist.id);
      
      artistUpserts.push({
        where: { artist_id: artist.id },
        update: { name: artist.name, artist_url: artist.external_urls.spotify },
        create: { artist_id: artist.id, name: artist.name, artist_url: artist.external_urls.spotify }
      });
    }

    // Collect track-artist relationships
    for (const artist of track.artists) {
      trackArtistRelations.push({
        where: { track_id_artist_id: { track_id: track.id, artist_id: artist.id } }, update: {},
        create: { track_id: track.id, artist_id: artist.id }
      });
    }

    // Collect album-artist relationships
    for (const artist of track.album.artists) {
      albumArtistRelations.push({
        where: { album_id_artist_id: { album_id: track.album.id, artist_id: artist.id } }, update: {},
        create: { album_id: track.album.id, artist_id: artist.id }
      });
    }

    // Collect play history
    playHistoryUpserts.push({
      where: { track_id_played_at: { track_id: track.id, played_at: new Date(played_at) } }, update: {},
      create: { track_id: track.id, played_at: new Date(played_at) }
    });
  }

  // Execute all operations in a transaction for better performance
  await prisma.$transaction(async (tx) => {
    const batchSize = 50;
    
    // Albums
    for (let i = 0; i < albumUpserts.length; i += batchSize) {
      const batch = albumUpserts.slice(i, i + batchSize);
      await Promise.all(batch.map(op => tx.spalbum.upsert(op)));
    }

    // Artists
    for (let i = 0; i < artistUpserts.length; i += batchSize) {
      const batch = artistUpserts.slice(i, i + batchSize);
      await Promise.all(batch.map(op => tx.spartist.upsert(op)));
    }

    // Tracks
    for (let i = 0; i < trackUpserts.length; i += batchSize) {
      const batch = trackUpserts.slice(i, i + batchSize);
      await Promise.all(batch.map(op => tx.sptrack.upsert(op)));
    }

    // Relationships and play history
    await Promise.all([
      ...trackArtistRelations.map(op => tx.sptrackartist.upsert(op)),
      ...albumArtistRelations.map(op => tx.spalbumartist.upsert(op)),
      ...playHistoryUpserts.map(op => tx.spplayhistory.upsert(op))
    ]);
  });

  // Fetch artist images and update common albums
  await Promise.all([
    updateArtistImages(Array.from(allArtistIds)),
    optimizedAssignCommonAlbumUrls()
  ]);

  // Aggregate daily stats for the unique dates in the ingested plays
  const uniqueDates = new Set<string>();
  for (const item of items as any[]) {
    const playDate = new Date(item.played_at);
    const dateStr = playDate.toISOString().split('T')[0];
    uniqueDates.add(dateStr);
  }

  for (const dateStr of uniqueDates) {
    const targetDate = new Date(`${dateStr}T00:00:00.000Z`);
    await aggregateDailyStats(targetDate);
  }

  console.log(`‚úÖ Ingested ${items.length} plays and updated stats for ${uniqueDates.size} unique dates`);
};
```

---

## Initial Table Population

Now I had a strong process for populating `spplayhistory`, `spalbum`, `sptrack`, `spartist`, `spalbumartist`, and `sptrackartist` in the database. I still had to take the previously merged data to push the last 8 years worth of listening history into the database. That was done with the following function with `pushPlayDetailsToDB()`.

<div data-component="CallOut" data-header="Coding Tips">

- I should have used `setTimeout` to space out the database operations on my super tiny DB...but I just let it rip.
- Monitor the token expiration time and refresh it as needed.
- Log progress after each chunk is processed to troubleshoot edge cases.

</div>


```
##Pushing Historical Play Details to DB
#canCollapse
async function pushPlayDetailsToDB() {                                    
  var inputPath = path.join(directory, `tools/trackHistoryData/finalSpotifyTrackDetails.json`);
  let trackDetails = await JSON.parse(fs.readFileSync(inputPath, 'utf-8'));

  const CHUNK_SIZE = 400;
  let cursor = 0;
  let chunkNumber = 1;

  while (cursor < trackDetails.length) {
    const dataChunk = trackDetails.slice(cursor, cursor + CHUNK_SIZE);

    log(`üöÄ Ingesting chunk #${chunkNumber} with ${dataChunk.length} tracks @ ${new Date().toISOString()}`);

    try {await ingestSpotifyPlays(true, {status: 200, data: dataChunk }); }
    catch (error) { log(`‚ùå Ingestion error in file chunk #${chunkNumber}: ${JSON.stringify(error)}`); }

    cursor += CHUNK_SIZE;
    chunkNumber++;
  }
  
  log('All files processed.');
}
```  

<br />

While that created and ensured that my "source of truth" pipeline was operating, I still needed to prepare the `yearlybucket`, `monthlybucket`, `daybucket` tables. Each yearbucket can only reference 12 month buckets, one month bucket can only reference 28-31 day buckets, and there could be no overlap or duplicate records. I chose to precreated these buckets starting at my first play record through then end of 2030.  I messed this up so bad and so many times. So much in fact that I had a process for starting over. The following functions created were used to populate the bucket tables:
- `purgebuckets()` My start over function.
- `buildAllBuckets()` Creates entries in the database for every possible year, month, and day bucket from 07/23/2017 to 12/31/2030.
- `linkBucketHierarchy()` Establishes the relations and table constraints between each of the day, month, and year buckets.
- `validateBuckets()` This checks for duplicate entries, missing links, incomplete buckets, etc.

<br />

```
##Bucket Creation Functions
#canCollapse
export async function purgeBuckets() {
  const logPath = path.join(process.cwd(), 'ingestion.log');
  const logStream = fs.createWriteStream(logPath, { flags: 'a' });
  function log(message: string) {
    const timestamp = new Date().toISOString();
    logStream.write(`[${timestamp}] ${message}\n`);
    console.log(message);
  }

  log('üß® Purging all stats and bucket tables...');
  // Delete stats first (FKs reference buckets)
  await prisma.artiststat.deleteMany({});
  await prisma.trackstat.deleteMany({});
//  await prisma.spdailyplaystats.deleteMany({}); This operates outside of the buckiting system
  // Delete buckets (lowest to highest)
  await prisma.daybucket.deleteMany({});
  await prisma.monthbucket.deleteMany({});
  await prisma.yearbucket.deleteMany({});
  log('‚úÖ All stats and bucket tables purged.');
}

export async function buildAllBuckets() {
  const logPath = path.join(process.cwd(), 'ingestion.log');
  const logStream = fs.createWriteStream(logPath, { flags: 'a' });
  function log(message: string) {
    const timestamp = new Date().toISOString();
    logStream.write(`[${timestamp}] ${message}\n`);
    console.log(message);
  }

  // Use first play event as start date
  let start = new Date('2017-07-23T00:00:00.000Z');
  const firstPlay = await prisma.spplayhistory.findFirst({ orderBy: { played_at: 'asc' } });
  if (firstPlay && firstPlay.played_at) {
    const d = new Date(firstPlay.played_at);
    start = new Date(Date.UTC(d.getUTCFullYear(), d.getUTCMonth(), d.getUTCDate()));
  }
  const end = new Date('2030-12-31T00:00:00.000Z');

  log('üî® Rebuilding all bucket tables...');

  // Year buckets
  const yearStarts = eachYearOfInterval({ start, end });
  const yearBucketInputs = yearStarts.map(yearStart => {
    const year = yearStart.getUTCFullYear();
    const range_start = new Date(Date.UTC(year, 0, 1));
    const range_end = new Date(Date.UTC(year, 11, 31, 23, 59, 59, 999));
    return { year, range_start, range_end };
  });
  await prisma.yearbucket.createMany({ data: yearBucketInputs, skipDuplicates: true });
  log(`‚úÖ Created/confirmed ${yearBucketInputs.length} yearbucket rows`);

  // Build year lookup
  const yearRows = await prisma.yearbucket.findMany();
  const yearLookup = new Map<number, number>(yearRows.map(y => [y.year, y.id]));

  // Month buckets
  const monthStarts = eachMonthOfInterval({ start, end });
  const monthBucketInputs = [];
  for (const monthStart of monthStarts) {
    const year = monthStart.getUTCFullYear();
    const month = monthStart.getUTCMonth() + 1;
    const range_start = new Date(Date.UTC(year, month - 1, 1));
    const range_end = new Date(Date.UTC(year, month, 0, 23, 59, 59, 999));
    const yearbucketid = yearLookup.get(year);
    if (!yearbucketid) {
      log(`‚ùå Skipping monthbucket for ${year}-${month}: no yearbucket found`);
      continue;
    }
    if (yearbucketid) {
      monthBucketInputs.push({ yearbucketid, month, range_start, range_end });
    } else {
      log(`‚ùå Skipping monthbucket for ${year}-${month}: no yearbucket found`);
    }
  }
  await prisma.monthbucket.createMany({ data: monthBucketInputs, skipDuplicates: true });
  log(`‚úÖ Created/confirmed ${monthBucketInputs.length} monthbucket rows`);

  // Build month lookup
  const monthRows = await prisma.monthbucket.findMany();
  // key: `${year}-${month}`
  const monthLookup = new Map<string, number>(monthRows.map(m => [`${m.yearbucketid}-${m.month}`, m.id]));

  // (Removed week bucket creation)

  // Day buckets
  const dayStarts = eachDayOfInterval({ start, end });
  // Build a lookup for monthbucket by yearbucketid and month
  const monthRowsForDays = await prisma.monthbucket.findMany();
  const monthLookupForDays = new Map<string, number>(monthRowsForDays.map(m => [`${m.yearbucketid}-${m.month}`, m.id]));
  const dayBucketInputs = dayStarts.map(date => {
    const year = date.getUTCFullYear();
    const month = date.getUTCMonth() + 1;
    // Find yearbucketid for this year
    const yearbucketid = yearLookup.get(year);
    const monthbucketid = monthLookupForDays.get(`${yearbucketid}-${month}`);
    return {
      start_date: startOfDay(date),
      bucket_scope: 'day',
      monthbucketid: monthbucketid!,
    };
  });
  await prisma.daybucket.createMany({ data: dayBucketInputs, skipDuplicates: true });
  log(`‚úÖ Created/confirmed ${dayBucketInputs.length} daybucket rows`);

  log('üéâ All bucket tables rebuilt.');
}

export async function linkBucketHierarchy() {
  const logPath = path.join(process.cwd(), 'ingestion.log');
  const logStream = fs.createWriteStream(logPath, { flags: 'a' });
  function log(message: string) {
    const timestamp = new Date().toISOString();
    logStream.write(`[${timestamp}] ${message}\n`);
    console.log(message);
  }

  log('üîó Linking bucket hierarchy...');

  const yearBuckets = await prisma.yearbucket.findMany();
  const monthBuckets = await prisma.monthbucket.findMany();
  const dayBuckets = await prisma.daybucket.findMany();

  // Link monthbucket ‚Üí yearbucket
  for (const month of monthBuckets) {
    const parent = yearBuckets.find(y =>
      month.range_start >= y.range_start && month.range_end <= y.range_end
    );
    if (parent && month.yearbucketid !== parent.id) {
      await prisma.monthbucket.update({
        where: { id: month.id },
        data: { yearbucketid: parent.id },
      });
      log(`monthbucket ${month.id} linked to yearbucket ${parent.id}`);
    }
  }

  // (Removed weekbucket linking)

  log('‚úÖ Bucket hierarchy linked via range containment');
}

export async function validateBuckets() {
  const logPath = path.join(process.cwd(), 'ingestion.log');
  const logStream = fs.createWriteStream(logPath, { flags: 'a' });
  function log(message: string) {
    const timestamp = new Date().toISOString();
    logStream.write(`[${timestamp}] ${message}\n`);
    console.log(message);
  }

  log('üîé Validating bucket hierarchy and date ranges...');

  // Define valid date range
  const validStart = new Date('2017-07-23T00:00:00.000Z');
  const validEnd = new Date('2030-12-31T23:59:59.999Z');

  let issues = 0;

  // Check yearbucket date range
  const yearBuckets = await prisma.yearbucket.findMany();
  for (const year of yearBuckets) {
    if (year.range_start < validStart || year.range_end > validEnd) {
      log(`‚ùå yearbucket ${year.id} out of range: ${year.range_start.toISOString()} - ${year.range_end.toISOString()}`);
      issues++;
    }
  }

  // Check monthbucket ‚Üí yearbucket and date range
  const monthBuckets = await prisma.monthbucket.findMany();
  for (const month of monthBuckets) {
    if (!month.yearbucketid) {
      log(`‚ùå monthbucket ${month.id} missing yearbucketid`);
      issues++;
    }
    if (month.range_start < validStart || month.range_end > validEnd) {
      log(`‚ùå monthbucket ${month.id} out of range: ${month.range_start.toISOString()} - ${month.range_end.toISOString()}`);
      issues++;
    }
  }

  // Check daybucket date range
  const dayBuckets = await prisma.daybucket.findMany();
  for (const day of dayBuckets) {
    if (day.start_date < validStart || day.start_date > validEnd) {
      log(`‚ùå daybucket ${day.id} out of range: ${day.start_date.toISOString()}`);
      issues++;
    }
  }

  if (issues === 0) {
    log('‚úÖ All bucket hierarchy links are valid.');
  } else {
    log(`‚ö†Ô∏è Validation complete: ${issues} issues found.`);
  }
}
```

## Aggregation

As plays were ingested I had to aggregate total play counts for all appropriate buckets, creating new `artiststat` and `trackstat` records. For example, if I fetched music I listened to on 9/28/2025 I'd need to update the yearly bucket for 2025, the monthly bucket for September of 2025, and the daily bucket for 9/28/2025. Updating those buckets would include creating (or updating) artiststat and trackstat records for every artist and track associated with those buckets. There were two steps to this, aggregating historical data and aggregating new data. I could restart this process as much as I needed because the source of truth, `spplayhistory`, was accurate and being updated regularly every 30 minutes.

- `updateBucketedStats(dryRun: boolean = false)` This function would go through the entire `spplayhistory` table and create aggregate counts for every bucket, every artist, and every track. I used a dryRun flag so that I could log random items as it went to validate what was happening before I actually added a bunch of data to the database. 
- `aggregateDailyStats(targetDate: Date)` This function aggregates data for a specific date. In my case I call it every 6 hours for the last three days. I chose three days to account for possible errors during previous runs, possible time zone issues, and if I realized during development I had to aggregate differently.

<br />

```
##aggregateDailyStats(targetDate)
#canCollapse
export const aggregateDailyStats = async (targetDate: Date): Promise<void> => {
  const dateStr = targetDate.toISOString().split('T')[0];
  const start = new Date(`${dateStr}T00:00:00.000Z`);
  const end = new Date(`${dateStr}T23:59:59.999Z`);
  const weekday = start.toLocaleDateString('en-US', { weekday: 'short' });

  // Delete existing daily play stats
  await prisma.spdailyplaystats.deleteMany({ where: { date: start } });

  // Fetch play history with track and artist info
  const plays = await prisma.spplayhistory.findMany({
    where: { played_at: { gte: start, lte: end } },
    include: { track: { include: { track_artists: true } } }
  });

  const hourlyPlays = Array(24).fill(0);
  const trackCounts = new Map<string, number>();
  const artistCounts = new Map<string, number>();
  const trackHourlyMap = new Map<string, number[]>();
  const artistHourlyMap = new Map<string, number[]>();

  for (const play of plays) {
    const hour = new Date(play.played_at).getUTCHours();
    hourlyPlays[hour]++;

    const trackId = play.track_id;
    trackCounts.set(trackId, (trackCounts.get(trackId) || 0) + 1);
    if (!trackHourlyMap.has(trackId)) trackHourlyMap.set(trackId, Array(24).fill(0));
    trackHourlyMap.get(trackId)![hour]++;

    for (const artist of play.track.track_artists) {
      const artistId = artist.artist_id;
      artistCounts.set(artistId, (artistCounts.get(artistId) || 0) + 1);
      if (!artistHourlyMap.has(artistId)) artistHourlyMap.set(artistId, Array(24).fill(0));
      artistHourlyMap.get(artistId)![hour]++;
    }
  }

  // Create overall daily play stats
  await prisma.spdailyplaystats.create({ data: { date: start, weekday, hourly_plays: hourlyPlays } });

  console.log(`‚úÖ Overall stats created for ${dateStr}`);

  // Find existing day bucket (daybuckets are pre-created up to 2030)
  // Compare by date string to handle timezone differences
  const dayBucket = await prisma.daybucket.findFirst({
    where: { 
      start_date: {
        gte: new Date(`${dateStr}T00:00:00.000Z`),
        lt: new Date(`${dateStr}T23:59:59.999Z`)
      }
    }
  });

  if (!dayBucket) {
    throw new Error(`Day bucket not found for ${dateStr}. Daybuckets should be pre-created up to 2030.`);
  }

  // Create stats using individual upserts (matching updateBucketedStats approach)
  for (const [artist_id, count] of artistCounts.entries()) {
    await prisma.artiststat.upsert({
      where: { artist_id_stat_date: { artist_id, stat_date: start } },
      update: { count, hourly_plays: artistHourlyMap.get(artist_id), daybucketid: dayBucket.id, bucket_scope: 'day' },
      create: { artist_id, count, hourly_plays: artistHourlyMap.get(artist_id), stat_date: start, daybucketid: dayBucket.id, bucket_scope: 'day' }
	});
  }

  for (const [track_id, count] of trackCounts.entries()) {
    await prisma.trackstat.upsert({
      where: { track_id_stat_date: { track_id, stat_date: start } },
      update: { count, hourly_plays: trackHourlyMap.get(track_id), daybucketid: dayBucket.id, bucket_scope: 'day' },
      create: { track_id, count, hourly_plays: trackHourlyMap.get(track_id), stat_date: start, daybucketid: dayBucket.id, bucket_scope: 'day' }
    });
  }

  console.log(`üì¶ Bucketed stats updated for ${dateStr} (daybucket: ${dayBucket.id})`);
};
```

<br />

```
## updateBucketedStats(dryRun: boolean = false)
#canCollapse
export async function updateBucketedStats(dryRun: boolean = false) {
  const logPath = path.join(process.cwd(), 'ingestion.log');
  const logStream = fs.createWriteStream(logPath, { flags: 'a' });
  function log(message: string) {
    const timestamp = new Date().toISOString();
    logStream.write(`[${timestamp}] ${message}\n`);
    console.log(message);
  }

  log('üîÑ Rebuilding all bucketed stats from raw play history...');
  log('‚ÑπÔ∏è Starting day bucket aggregation...');

  // ...existing code...

  // Aggregate day stats from play history
  const dayBuckets = await prisma.daybucket.findMany({});
  for (const day of dayBuckets) {
    log(`üìÖ Aggregating stats for daybucket ${day.id} (${day.start_date.toISOString()})`);
    const plays = await prisma.spplayhistory.findMany({
      where: { played_at: { gte: day.start_date, lt: new Date(day.start_date.getTime() + 24 * 60 * 60 * 1000) } },
      include: { track: { include: { track_artists: true } } }
    });
    log(`üîé Found ${plays.length} plays for daybucket ${day.id}`);
    const artistCounts = new Map<string, number>();
    const trackCounts = new Map<string, number>();
    const hourlyArtistPlays: Map<string, number[]> = new Map();
    const hourlyTrackPlays: Map<string, number[]> = new Map();
    for (const play of plays) {
      trackCounts.set(play.track_id, (trackCounts.get(play.track_id) || 0) + 1);
      if (!hourlyTrackPlays.has(play.track_id)) hourlyTrackPlays.set(play.track_id, Array(24).fill(0));
      hourlyTrackPlays.get(play.track_id)![new Date(play.played_at).getUTCHours()]++;
      for (const ta of play.track.track_artists) {
        artistCounts.set(ta.artist_id, (artistCounts.get(ta.artist_id) || 0) + 1);
        if (!hourlyArtistPlays.has(ta.artist_id)) hourlyArtistPlays.set(ta.artist_id, Array(24).fill(0));
        hourlyArtistPlays.get(ta.artist_id)![new Date(play.played_at).getUTCHours()]++;
      }
    }
    log(`üé® Artist counts for daybucket ${day.id}: ${JSON.stringify(Array.from(artistCounts.entries()))}`);
    log(`üéµ Track counts for daybucket ${day.id}: ${JSON.stringify(Array.from(trackCounts.entries()))}`);

    for (const [artist_id, count] of artistCounts.entries()) {
      log(`‚¨ÜÔ∏è Upserting artiststat for artist ${artist_id} in daybucket ${day.id} (count: ${count})`);
      if (!dryRun) {
        await prisma.artiststat.upsert({
          where: { artist_id_stat_date: { artist_id, stat_date: day.start_date } },
          update: { count, hourly_plays: hourlyArtistPlays.get(artist_id), daybucketid: day.id, bucket_scope: 'day' },
          create: { artist_id, count, hourly_plays: hourlyArtistPlays.get(artist_id), stat_date: day.start_date, daybucketid: day.id, bucket_scope: 'day' },
        });
      }
    }

    for (const [track_id, count] of trackCounts.entries()) {
      log(`‚¨ÜÔ∏è Upserting trackstat for track ${track_id} in daybucket ${day.id} (count: ${count})`);
      if (!dryRun) {
        await prisma.trackstat.upsert({
          where: { track_id_stat_date: { track_id, stat_date: day.start_date } },
          update: { count, hourly_plays: hourlyTrackPlays.get(track_id), daybucketid: day.id, bucket_scope: 'day' },
          create: { track_id, count,hourly_plays: hourlyTrackPlays.get(track_id), stat_date: day.start_date, daybucketid: day.id, bucket_scope: 'day' }, 
        });
      }
    }
    log(`‚úÖ Updated daybucket ${day.id} stats.`);
  }

  log('‚ÑπÔ∏è Starting month bucket aggregation...');
  // Only process monthbuckets with id >= 2801
  let monthBuckets = await prisma.monthbucket.findMany({ where: { id: { gte: 2801 } }, orderBy: { id: 'asc' } });
  // Cleanup: Ensure yearbucketid is null for all month-level stats before aggregation
  await prisma.artiststat.updateMany({
    where: { monthbucketid: { not: null } },
    data: { yearbucketid: null }
  });
  await prisma.trackstat.updateMany({
    where: { monthbucketid: { not: null } },
    data: { yearbucketid: null }
  });
  for (const month of monthBuckets) {
    log(`üìÜ Aggregating stats for monthbucket ${month.id} (${month.range_start.toISOString()})`);
    const dayBuckets = await prisma.daybucket.findMany({ where: { monthbucketid: month.id } });
    const dayBucketIds = dayBuckets.map(db => db.id);
    log(`üîé Found ${dayBucketIds.length} daybuckets for monthbucket ${month.id}`);
    log(`üóìÔ∏è Daybucket dates for monthbucket ${month.id}: ${dayBuckets.map(db => db.start_date.toISOString()).join(', ')}`);
    // ...existing code...
    const artistStats = await prisma.artiststat.findMany({ where: { daybucketid: { in: dayBucketIds } } });
    // Patch: Group and sum by (artist_id, monthbucketid) based on stat_date
    const artistMonthMap = new Map<string, { artist_id: string, monthbucketid: number, count: number }>();
    for (const stat of artistStats) {
      if (!stat.artist_id || !stat.stat_date) continue;
      // Find correct monthbucket for stat_date
      const mb = monthBuckets.find(mb => mb.range_start <= stat.stat_date && mb.range_end >= stat.stat_date);
      if (!mb) continue;
      const key = `${stat.artist_id}|${mb.id}`;
      if (!artistMonthMap.has(key)) {
        artistMonthMap.set(key, { artist_id: stat.artist_id, monthbucketid: mb.id, count: 0 });
      }
      artistMonthMap.get(key)!.count += stat.count;
    }
    log(`üé® Artist counts for monthbucket ${month.id}: ${JSON.stringify(Array.from(artistMonthMap.values()).filter(a => a.monthbucketid === month.id))}`);
    const trackStats = await prisma.trackstat.findMany({ where: { daybucketid: { in: dayBucketIds } } });
    // Patch: Group and sum by (track_id, monthbucketid) based on stat_date
    const trackMonthMap = new Map<string, { track_id: string, monthbucketid: number, count: number }>();
    for (const stat of trackStats) {
      if (!stat.track_id || !stat.stat_date) continue;
      const mb = monthBuckets.find(mb => mb.range_start <= stat.stat_date && mb.range_end >= stat.stat_date);
      if (!mb) continue;
      const key = `${stat.track_id}|${mb.id}`;
      if (!trackMonthMap.has(key)) {
        trackMonthMap.set(key, { track_id: stat.track_id, monthbucketid: mb.id, count: 0 });
      }
      trackMonthMap.get(key)!.count += stat.count;
    }
    log(`üéµ Track counts for monthbucket ${month.id}: ${JSON.stringify(Array.from(trackMonthMap.values()).filter(t => t.monthbucketid === month.id))}`);
    // Insert artiststat rows (one per artist_id, monthbucketid)
    const artistRows = Array.from(artistMonthMap.values()).filter(a => a.monthbucketid === month.id).map(({ artist_id, monthbucketid, count }) => ({
      artist_id,
      count,
      bucket_scope: 'month',
      stat_date: month.range_start,
      monthbucketid
      // yearbucketid intentionally omitted for month-level stats
    }));
    log(JSON.stringify(artistRows[0]));
    // Batch upsert for artiststat
    if (!dryRun && artistRows.length > 0) {
      await prisma.artiststat.createMany({ data: artistRows, skipDuplicates: true });
      log(`[Prisma] Batch inserted trackstat for monthbucket=${month.id}, rows=${artistRows.length}`);
    }
    // Batch upsert for trackstat
    const trackRows = Array.from(trackMonthMap.values()).filter(t => t.monthbucketid === month.id).map(({ track_id, monthbucketid, count }) => ({
      track_id,
      count,
      bucket_scope: 'month',
      stat_date: month.range_start,
      monthbucketid
      // yearbucketid intentionally omitted for month-level stats
    }));
    if (!dryRun && trackRows.length > 0) {
      await prisma.trackstat.createMany({ data: trackRows, skipDuplicates: true });
      log(`[Prisma] Batch inserted trackstat for monthbucket=${month.id}, rows=${trackRows.length}`);
    }
    log(`‚úÖ Updated monthbucket ${month.id} stats.`);
  // Cleanup: Ensure yearbucketid is null for all month-level stats for this monthbucket, after upserts
  await prisma.artiststat.updateMany({ where: { monthbucketid: month.id }, data: { yearbucketid: null } });
  await prisma.trackstat.updateMany({ where: { monthbucketid: month.id }, data: { yearbucketid: null } });
  log(`üßπ Cleaned up yearbucketid for monthbucket ${month.id}`);
  }


  log('‚ÑπÔ∏è Starting year bucket aggregation...');
  const yearBuckets = await prisma.yearbucket.findMany({});
  for (const year of yearBuckets) {
    log(`üìÖ Aggregating stats for yearbucket ${year.id} (${year.range_start.toISOString()})`);
    const months = await prisma.monthbucket.findMany({ where: { yearbucketid: year.id } });
    const monthIds = months.map(mb => mb.id);
    log(`üîé Found ${monthIds.length} monthbuckets for yearbucket ${year.id}`);
    const artistStats = await prisma.artiststat.findMany({ where: { monthbucketid: { in: monthIds }, bucket_scope: 'month' } });
    const artistCounts = new Map<string, number>();
    for (const stat of artistStats) {
      artistCounts.set(stat.artist_id, (artistCounts.get(stat.artist_id) || 0) + stat.count);
    }
    log(`üé® Artist counts for yearbucket ${year.id}: ${JSON.stringify(Array.from(artistCounts.entries()))}`);
    const trackStats = await prisma.trackstat.findMany({ where: { monthbucketid: { in: monthIds }, bucket_scope: 'month' } });
    const trackCounts = new Map<string, number>();
    for (const stat of trackStats) {
      trackCounts.set(stat.track_id, (trackCounts.get(stat.track_id) || 0) + stat.count);
    }
    log(`üéµ Track counts for yearbucket ${year.id}: ${JSON.stringify(Array.from(trackCounts.entries()))}`);
    for (const [artist_id, count] of artistCounts.entries()) {
      log(`‚¨ÜÔ∏è Upserting artiststat for artist ${artist_id} in yearbucket ${year.id} (count: ${count})`);
      if (!dryRun) {
        await prisma.artiststat.upsert({
          where: {
            artist_id_yearbucketid: {
              artist_id,
              yearbucketid: year.id,
            },
          },
          update: { count, yearbucketid: year.id, bucket_scope: 'year', stat_date: year.range_start },
          create: {
            artist_id,
            count,
            bucket_scope: 'year',
            stat_date: year.range_start,
            yearbucketid: year.id,
          },
        });
      }
    }
    for (const [track_id, count] of trackCounts.entries()) {
      log(`‚¨ÜÔ∏è Upserting trackstat for track ${track_id} in yearbucket ${year.id} (count: ${count})`);
      if (!dryRun) {
        await prisma.trackstat.upsert({
          where: {
            track_id_yearbucketid: {
              track_id,
              yearbucketid: year.id,
            },
          },
          update: { count, yearbucketid: year.id, bucket_scope: 'year', stat_date: year.range_start },
          create: {
            track_id,
            count,
            bucket_scope: 'year',
            stat_date: year.range_start,
            yearbucketid: year.id,
          },
        });
      }
    }
    log(`‚úÖ Updated yearbucket ${year.id} stats.`);
  }

  log('üéâ All bucketed stats rebuilt from raw play history.');
}
```

---

## Routine Data Polling

Okay, so I have created the functions to ingest data, populate the database, and aggregate stats. I just had to make sure that this was done routinely and on a schedule. I will point out that I could have used serverless functions with a third party cron service, which the hosting service that I am using (Render.com) offers but I stubbornly knew it could be done with the tech stack I was using. More on that later. NextJS in of itself isn't designed to run cron jobs but because it is built on top of Node.js I leveraged Node.js to set up run background services.

- I added `express` and `node-cron` to my NextJS app.
- Create a custom `server.js` file to start an Express server and modify the `start` script in `package.json` to run `node server.js` instead of `next start`.
- Within `server.js` there are two cron jobs
  * 1) Runs every 30 minutes to ingest new Spotify history
  * 2) Runs every 6 hours to aggregate daily stats for the last three days.
- Built safeguards for dev mode to prevent overlapping executions. 

<br />

```##server.js
#canCollapse
import express from 'express';
import next from 'next';
import { schedule } from 'node-cron'; 

const port = parseInt(process.env.PORT, 10) || 3000;
const dev = process.env.NODE_ENV !== 'production';
const app = next({ dev });
const handle = app.getRequestHandler();

app.prepare().then(() => {
  const server = express();
  const domain = dev ? 'http://localhost:3000' : 'https://lucas.untethered4life.com';
  const historyurl = `${domain}/api/getspotifyhistory`;
  const dailyurl = `${domain}/api/aggregatedailystats`;

  console.log(`Cron job will POST to: ${historyurl} and ${dailyurl}`);
  console.log(`Running in ${dev ? 'development' : 'production'} mode`);
  let isRunning = false;

  async function runSpotifyIngestion() {
    try {
      const response = await fetch(historyurl, { method: 'POST', headers: { 'x-cron-token': process.env.CRON_SECRET } })
      const data = await response.json();
      
      if (!response.ok) { throw new Error(`HTTP ${response.status}: `, data); }
      console.log('‚úÖ Scheduled spotify ingestion response:', data);
    
    } catch (error) { console.error('‚ùå Error in scheduled task:', error); }
  }

  async function aggregateDailyStats() {
    try {
      const response = await fetch(dailyurl, { method: 'POST', headers: { 'x-cron-token': process.env.CRON_SECRET } })
      const data = await response.json();
      
      if (!response.ok) { throw new Error(`HTTP ${response.status}: `, JSON.stringify(data)); }
      console.log('‚úÖ Scheduled aggregate daily stats response:', data);
    
    } catch (error) { console.error('‚ùå Error in scheduled task:', error); }
  }

  if (!dev) {
    schedule('*/30 * * * *', async () => {
      if (isRunning) { console.warn(`‚ö†Ô∏è Skipping run ‚Äî previous job still running`); return; }

      isRunning = true;
      
      console.log(`‚è±Ô∏è Attempting data ingestion at ${new Date().toISOString()}`);
      try { await runSpotifyIngestion(); } finally { isRunning = false; }
    });

    schedule('10 */6 * * *', async () => {
      console.log(`‚è±Ô∏è Attempting daily stats aggregation at ${new Date().toISOString()}`);
      await aggregateDailyStats();
    });
  } 

  server.all('*', (req, res) => { return handle(req, res); });
  server.listen(port, (err) => {
    if (err) throw err;
    console.log(`> Ready on ${domain}`);
  });
}); 
```

<br />

Okay....so we have all of the data ingested, aggregated, and updated on a regular basis. Can we finally get that data to the front end? Nope, not yet, but I was almost there.
- Build the back end API routes to fetch this data.
- Build the front end to display it.

<br />

I thought I was almost there. The back end API route to fetch data was the most difficult part of this entire project and took me roughly a week just for this function. Problems that I had:
- Long processing times (10+ seconds initially, ouch).
- Memory usage spikes, this code took down my deployed instance a handful of times.
- Database locks, timeouts, and refused connections.
- Random brain fries, where I just couldn't continue to process what was happening myself.
- AI misdirecting me or writing code that was horribly wrong (admittedly AI also provided code that was solid and incredibly complex as well).
- Having to take a shower, eat, feed my dog, and occasionally find a natural source of vitamin D delayed my progress too.
- With a lot of caffeine, limited sleep, and my friends....ChatGPT, Copilot, and Claude this is what I came up with:

<br />

```
##getSpotifyStatsByDateRange(startDate, endDate)
#canCollapse
export const getSpotifyStatsByDateRange = async (startDate: Date, endDate: Date): Promise<SpotifyDataResponseProps> => {
  try {
    const startTime = Date.now();

    // Use raw SQL for much faster bucket-aware stats aggregation
    const [dailyStats, topArtistsRaw, topTracksRaw] = await Promise.all([
      prisma.spdailyplaystats.findMany({ 
        where: { date: { gte: startDate, lte: endDate } },
        select: { date: true, weekday: true, hourly_plays: true }
      }),
      
      // Raw SQL for top artists - much faster than Prisma OR queries
      prisma.$queryRaw`
        SELECT 
          a.artist_id,
          a.name,
          a.image_url,
          a.artist_url,
          COALESCE(SUM(ast.count), 0) as total_count
        FROM spartist a
        INNER JOIN artiststat ast ON a.artist_id = ast.artist_id
        WHERE ast.stat_date >= ${startDate} AND ast.stat_date <= ${endDate}
        GROUP BY a.artist_id, a.name, a.image_url, a.artist_url
        ORDER BY total_count DESC
        LIMIT 10
      `,
      
      // Raw SQL for top tracks - much faster than Prisma OR queries  
      prisma.$queryRaw`
        SELECT 
          t.track_id,
          t.title,
          t.isrc,
          t.song_url,
          t.explicit,
          t.release_date,
          t.album_id,
          a.name as album_name,
          a.image_url as album_image,
          a.release_date as album_release_date,
          ca.album_id as common_album_id,
          ca.name as common_album_name,
          ca.image_url as common_album_image,
          ca.release_date as common_album_release_date,
          STRING_AGG(ar.name, ', ' ORDER BY ar.name) as artists,
          COALESCE(SUM(ts.count), 0) as total_count
        FROM sptrack t
        INNER JOIN trackstat ts ON t.track_id = ts.track_id
        LEFT JOIN spalbum a ON t.album_id = a.album_id
        LEFT JOIN spalbum ca ON t.common_album_id = ca.album_id
        LEFT JOIN sptrackartist ta ON t.track_id = ta.track_id
        LEFT JOIN spartist ar ON ta.artist_id = ar.artist_id
        WHERE ts.stat_date >= ${startDate} AND ts.stat_date <= ${endDate}
        GROUP BY t.track_id, t.title, t.isrc, t.song_url, t.explicit, t.release_date, t.album_id, 
                 a.name, a.image_url, a.release_date, ca.album_id, ca.name, ca.image_url, ca.release_date
        ORDER BY total_count DESC
        LIMIT 10
      `
    ]);

    console.log(`Raw query completed in ${Date.now() - startTime}ms`);
 
    // Process the results from raw SQL queries
    const playFrequency = dailyStats.map((stat) => ({
      date: stat.date.toISOString().substring(0, 10),
      weekday: stat.weekday,
      hourly_plays: Array.isArray(stat.hourly_plays) ? stat.hourly_plays : Array(24).fill(0)
    }));

    // Format top artists from raw SQL results
    const topArtists = (topArtistsRaw as any[]).map(row => ({
      artistId: row.artist_id,
      name: row.name || '',
      image: row.image_url || 'images/spotify-icon.svg',
      artist_url: row.artist_url || '',
      count: Number(row.total_count)
    }));

    // Format top tracks from raw SQL results  
    const topTracks = (topTracksRaw as any[]).map(row => ({
      trackId: row.track_id,
      isrc: row.isrc || '',
      album: {
        albumId: row.album_id || '',
        name: row.album_name || '',
        release_date: row.album_release_date || new Date(),
        image: row.album_image || 'images/spotify-icon.svg'
      },
      common_album: {
        albumId: row.common_album_id || row.album_id || '',
        name: row.common_album_name || row.album_name || '',
        image: row.common_album_image || row.album_image || 'images/spotify-icon.svg',
        release_date: row.common_album_release_date || row.album_release_date || new Date()
      },
      artists: row.artists || '',
      songUrl: row.song_url || '',
      title: row.title || '',
      release_date: row.release_date || new Date(),
      explicit: Boolean(row.explicit),
      count: Number(row.total_count)
    }));

    console.log(`Data processing completed in ${Date.now() - startTime}ms`);

    // Get accurate metadata counts for all tracks/artists in date range
    const [totalCountsRaw] = await Promise.all([
      prisma.$queryRaw`
        SELECT 
          COUNT(DISTINCT t.track_id) as total_track_count,
          COUNT(DISTINCT a.artist_id) as total_artist_count,
          COALESCE(SUM(CASE WHEN t.explicit = true THEN ts.count ELSE 0 END), 0) as explicit_count,
          COALESCE(SUM(ts.count), 0) as total_play_count
        FROM trackstat ts
        INNER JOIN sptrack t ON ts.track_id = t.track_id
        INNER JOIN sptrackartist ta ON t.track_id = ta.track_id
        INNER JOIN spartist a ON ta.artist_id = a.artist_id
        WHERE ts.stat_date >= ${startDate} AND ts.stat_date <= ${endDate}
      `
    ]);

    const totalCounts = (totalCountsRaw as any[])[0];
    const totalTrackCount = Number(totalCounts.total_track_count);
    const totalArtistCount = Number(totalCounts.total_artist_count);
    const explicitCount = Number(totalCounts.explicit_count);
    const totalPlayCount = Number(totalCounts.total_play_count);
    const percentExplicit = totalPlayCount > 0 ? parseFloat(((explicitCount / totalPlayCount) * 100).toFixed(1)) : 0.0;

    // Get heatmap data for top artists and tracks only (much more efficient)
    let artistHeatmaps: HeatmapFrequencies[] = [];
    let trackHeatMaps: HeatmapFrequencies[] = [];
    
    if (topArtists.length > 0 || topTracks.length > 0) {
      const topArtistIds = topArtists.map(a => a.artistId);
      const topTrackIds = topTracks.map(t => t.trackId);
      
      // Get detailed hourly data only for top items
      const [heatmapArtistStats, heatmapTrackStats] = await Promise.all([
        prisma.artiststat.findMany({
          where: { 
            artist_id: { in: topArtistIds },
            stat_date: { gte: startDate, lte: endDate },
            bucket_scope: 'day'
          },
          select: { artist_id: true, stat_date: true, hourly_plays: true }
        }),
        prisma.trackstat.findMany({
          where: { 
            track_id: { in: topTrackIds },
            stat_date: { gte: startDate, lte: endDate },
            bucket_scope: 'day'  
          },
          select: { track_id: true, stat_date: true, hourly_plays: true }
        })
      ]);

      // Format heatmaps efficiently
      artistHeatmaps = heatmapArtistStats.map(stat => ({
        date: stat.stat_date.toISOString().split('T')[0],
        weekday: format(stat.stat_date, 'EEEE'),
        hourly_plays: Array.isArray(stat.hourly_plays) && stat.hourly_plays.length === 24 
          ? stat.hourly_plays 
          : Array(24).fill(0),
        artistId: stat.artist_id,
        trackId: undefined
      }));

      trackHeatMaps = heatmapTrackStats.map(stat => ({
        date: stat.stat_date.toISOString().split('T')[0],
        weekday: format(stat.stat_date, 'EEEE'),
        hourly_plays: Array.isArray(stat.hourly_plays) && stat.hourly_plays.length === 24 
          ? stat.hourly_plays 
          : Array(24).fill(0),
        artistId: undefined,
        trackId: stat.track_id
      }));
    }

    console.log(`Query completed in ${Date.now() - startTime}ms`);

    return { 
      status: 200, 
      data: {
        topArtists,
        topTracks,
        playFrequency,
        artistHeatmaps,
        trackHeatMaps,
        meta: {
          totalTrackCount,
          totalArtistCount,
          percentExplicit
        }
      }
    };
  } catch (error) {
    console.error('Error in getSpotifyStatsByDateRange:', error);
    return { 
      status: 500, 
      data: { 
        topArtists: [], 
        topTracks: [], 
        playFrequency: [], 
        artistHeatmaps: [], 
        trackHeatMaps: [], 
        meta: { totalTrackCount: 0, totalArtistCount: 0, percentExplicit: 0 } 
      } 
    };
  }
};
```

---

# Deploying on Render
<span id="deploying" name="Deploying" data-toc></span>

Previously, I have hosted projects on AWS. I've used EC2 instances, Lambda functions, S3 buckets, RDS databases, and a host of other AWS services. When I first started using AWS I geeked out on all the different things that you could do. 

Over time I realized:
- I was spending more time managing infrastructure than building actual applications.
- It became more and more difficult to estimate costs, every small tidbit of processing, storage, or data transferring had an associated cost.
- It was easy to make unrecoverable mistakes. Like when I installed my first NextJS application with a simple template on a t2.micro instance. The installation alone used so much of the instance's resources that it couldn't create a page file to migrate to a larger instance...I just had to start over.

With that, I did some research and found Render.com. Render is a modern cloud platform that makes it easy to deploy web applications, APIs, and databases with minimal configuration. Whenever I read statements about how easy something is technically, I am always skeptical. But I gave them a try and was pleasently surprised.

## Instance Creation

<div className="grid grid-cols-2 gap-4">
  <div>

Once you have created an account get started by selecting New -> Web Service. After that, simply connect your GitHub or GitLab account and select a repository. Render automatically detects the type of application, in this case NextJS, and then fills in the typical build and start commands for you. Select an instance type, region, and enter your environment variables.
  
  </div>
  <div>
    <img src="/images/projects/spotify/render-setup-1.webp" alt="Render Setup 0" width="100"/>
  </div>
</div>

<img src="/images/projects/spotify/render-setup-2.webp" alt="Render Setup 2" />

I started with a Starter instance as I didn't expect I'd require much for a profile site.

<img src="/images/projects/spotify/render-setup-3.webp" alt="Render Setup 3" />

Render gave me a generated domain, `profile-site-rt4b:10000`, to access the site immediately.I went ahead and added my own custom domain. Render gave me the specific DNS entries that I needed to configure with my DNS provider. Right within the Render dashboard was a link to documentation for what updating DNS records may look like depending on your DNS provider. A simply verification link was available to check for DNS propegation and once the record was verified...you'll see the green verified check mark! I added a test custom domain to demonstrate what I mean.

<img src="/images/projects/spotify/render-setup-6.webp" alt="Render Setup 6" />

Repeat that same process to fire up PostgreSQL database, I chose a Basic-256mb instance. They provide both an internal and an external connections string, which using the external connection string was helpful for local development and having a more direct connection string for your deployed application is a nice benefit.

<img src="/images/projects/spotify/render-setup-4.webp" alt="Render Setup 4" />

<img src="/images/projects/spotify/render-setup-5.webp" alt="Render Setup 5" />

I had never used their services before, and it took roughly 10 minutes to build and deploy my application. That is not an exagerated statement!

## Deployment Pipeline

Because I connected my GitHub repository to Render, every time I push to the main branch it automatically triggers a new build and deployment. Render provides a detailed build log so you can see exactly what is happening during the build process. By navigating to the Events tab for your service you can see the status of each deployment. Each row includes a link to the build logs, a link to the exact commit that triggered the deployment, debug logs if needed, and the ability to rollback to a previous deployment if something else went wrong.

Now I did have a few hiccups with the Starter instances. Once I got into the thick of this Spotify Heatmap project I quickly realized that the Starter instances for the web service and database were not going to cut it. With a few clicks and a couple minutes, I was able to scale up to a Standard instance for both the web service and the database with not additional work required from me. Also, Render uses fairly aggressive caching for builds, so after the initial build subsequent builds were very quick but my changes wouldn't show up. A handful of times I had to clear the build cache and re-deploy....but again, this was very easy to accomplish. I didn't have to bury myself in multiple nested levels of documentation or do any sort of exhaustive debugging to figure out what was going  happening. 

Everything I needed to do was a few clicks, literally.

## Monitoring & Alerts

As I was ingesting large sets of data over and over again I made use of the monitoring Render provides within their metrics dashboards. For example, you can see very clearly when my backend cron job runs every 30 minutes to ingest new Spotify history data.

As iterated through the building of the yearbuckets, monthbuckets, and daybuckets I used the Top Queries table listed in the database metrics dashboard to identify which queries were taking the longest to execute and determine if/how I could optimize them.

<img src="/images/projects/spotify/render-monitoring-1.webp" alt="Render Monitoring 1" width="600"/>

## Final opinion on Render

One gripe that I did have that didn't really fit into the other sections was that I knew what Node.js, Express, and NextJS were capable of. With that, Render's SEO game is strong! Whether I did a Google search, asked my inline AI coding assistant, or independent AI assistant for instructions for running an Express server alongside a NextJS application on Render EVERYTHING, and I mean EVERYTHING suggested I create a new Background Worker or Cron Job service within Render. Creating a cron job service is nearly identical to creating a web service, you select a repo, it auto populates most fields, but there are two differences.

- You enter a cron expression.
- The cost is converted from a monthly rate to a per minute rate.

Essentially, Render will build your code in a seperate container at the scheduled time and run it. However, I already had a Node service running where I was paying for dedicated resources monthly and I didn't want to pay for two separate instances. I wanted to leverage the existing instance that I was already paying for. 

Now, before you all think I am knocking Render, I'm not. Render doesn't stop you from getting in the weeds with your infrastructure if you want to. They also make it really easy to do what you need to do without the complications typically associated with more direct infrastructure management. Less time worrying about infrastructure and more time shipping code is exactly the value propisition that Render offers, and I am sold, they deliver that promise 100%. There are a slew of other features that I didn't even touch on like blueprints (their version of Infrastructure as Code), private links, log forwarding, metrics streaming, health check services, and a whole lot more. The next project I jump into that is more than just a profile site will definitely be hosted on Render and I look forward to more fully utilizing their service.

---

# Rendering
<span id="rendering" name="Rendering" data-toc></span>

Wow, it took a lot of pre-work to get here. To that final magical moment where all of the peices come together and you see the result of your labor. Or...you don't and you go back and forth debugging for hours on end, I'd like to think my code always works first time around, but I digress. Rendering turned out to be the easiest part of this entire project (except for deploying on Render, that was much easier actually).

---

# Lessons & Bloopers
<span id="lessons-bloopers" name="Lessons & Bloopers" data-toc></span>

## Prisma Studio

This was my first time using Prisma. Prisma is great as an ORM in Javascript based frameworks. It is very easy to use and the type safety is a huge plus. However, I ran into a few issues with Prisma Studio. It was just plain wrong and often!

It would show data that wasn't there and not show data that was there. For example, when I was validating the creation of my bucketed stats it would show that there were no `artiststats` or `trackstats` for a given day. This led my down some incorrect troubleshooting paths, until I finally reliazed that the data was in fact there when I queried the database directly.

<img src="/images/projects/spotify/prisma-studio-1.webp" alt="Prisma Studio Issue 1" />

---

## Count Everything, Always!

At some point, AI decided that the buckets didn't really matter, especially because we can just fetch all of them, I mean, why not!

```
## Fetching all buckets and ALL precounted stats, ALL buckets, ALL tracks, and ALL artists.
const [artistStats, trackStats] = await Promise.all([
  prisma.artiststat.findMany({
    where: { OR: [
      { bucket_scope: 'year', yearbucketid: { in: yearBucketIds } },
      { bucket_scope: 'month', monthbucketid: { in: monthBucketIds } },
      { bucket_scope: 'week', weekbucketid: { in: weekBucketIds } },
      { bucket_scope: 'day' }
    ]
    },
    include: { artist: true }
  }),
  prisma.trackstat.findMany({
    where: { OR: [
      { bucket_scope: 'year', yearbucketid: { in: yearBucketIds } },
      { bucket_scope: 'month', monthbucketid: { in: monthBucketIds } },
      { bucket_scope: 'week', weekbucketid: { in: weekBucketIds } },
      { bucket_scope: 'day' }
    ]
    },
    include: { track: { include: { album: true, common_album: true, track_artists: { include: { artist: true } } } } }
  })
]);
```

This code gave me an opportunity to see how responsive Render's alerting services were, and I can tell you that they are very responsive. This bit of code would bring down the Starter instance and small database I began with almost instantly.

## Double wrapped

My `src/pages/api` routes return data wrapped in a standard response object like this:

```
{ status: number;  data: SpotifyDataResponseProps; }
```

Well from a backend service I accidentally returned data as if it was an API route:
  
```
res.status(200).json({ status: 200, data: { ... } });
``` 

which resulted in a double wrap return like this:

```
{
  status: 200,
  data: {
    status: 200,
    data: { ...actual data... }
  }
}
``` 

That little nugget cost me serious debugging time.

---

## Something doesn't look right...

As I was building the front end I kept seeing things that just didn't look right...

<div className="flex flex-col md:flex-row gap-4 my-4">
  <img src="/images/projects/spotify/funnyrender1.webp" alt="Funny Heatmap Render 1" width="400" />
  <img src="/images/projects/spotify/funnyrender2.webp" alt="Funny Heatmap Render 2" width="400" />
  <img src="/images/projects/spotify/funnyrender3.webp" alt="Funny Heatmap Render 3" width="67" />
  <img src="/images/projects/spotify/funnyrender4.webp" alt="Funny Heatmap Render 4" width="200" />
</div>

---

## Arguing with my AI

I had some, well, interesting conversations with my AI assistants. Like when AI tells me how I incorrectly wrote something that I didn't write...

<img src="/images/projects/spotify/ai-argument.webp" alt="AI Argument" width="600"/>

Or, when your AI assistance is convinced that they know...

<img src="/images/projects/spotify/ai-persistance-1.webp" alt="AI Persistance Example 1" width="600"/>
<img src="/images/projects/spotify/ai-persistance-2.webp" alt="AI Persistance Example 2" width="600"/>
<img src="/images/projects/spotify/ai-persistance-3.webp" alt="AI Persistance Example 3" width="600"/>

---

# Future Enhancements
<span id="future-enhancements" name="Future Enhancements" data-toc></span>

## ISRC Code Integration

## Specific Hour Details

## Track Features

## Social Sharing
