---
id: 1
title: 'Spotify Usage Heatmap'
stacks: ['JavaScript', 'TypeScript', 'Node.js', 'React', 'Next.js', 'PostgreSQL', 'Prisma', 'TailwindCSS']
slug: 'spotify-usage-heatmap'
image: 'https://lucas.untethered4life.com//images/projects/spotify/spotify-heatmap-banner.png'
link_demo: 'https://lucas.untethered4life.com/dashboard'
link_github: 'https://lucas.untethered4life.com/dashboard'
created_at: '2025-09-25'
updated_at: '2025-09-25'
---

# Spotify Usage Heatmap

Many developers have likely seen the classic Github repo submit history map. A visualization, where each square in A GRID represents a day. The color intensity of each square indicating the number of commits made on that day. Darker squares represent days with more commits, while lighter squares represent days with fewer commits. A visualization that helps to quickly review coding activity over time.

After including a "Now Playing" component on my own profile site, which was pulled from a Github repository, my curiosity got the best of me and I delved into a rabbit hole of REST API calls, relational database sctructures, data transormations, jq bash scripts, server side cron jobs, and a whole lot more. The end result was a dashboard that visualizes my own music listening habits over time, similar to that well known Github commit history map most developers today know.

In this article I will describe:
- The steps I took to build the dashboard.
- The challenges I faced along the way.
- The technologies I used to bring it all together.
- And the overall lessons and insights I gained from the experience.

## <span id="sample-data" name="Sample Data"></span>

> Step 1: Sample Data

- Create a [Spotify Developer account](https://developer.spotify.com/), register an application, and obtain necessary token to make API calls. For those that want to try this at home, the steps for authenticating with Spotify via the API can be found [HERE](https://developer.spotify.com/documentation/web-api/tutorials/code-flow).

- Call the Spotify API using Insomnia.

```
##Request
#canCollapse
curl --request GET \
  --url 'https://api.spotify.com/v1/me/player/recently-played?limit=50' \
  --header 'Authorization: Bearer token' \
  --header 'Content-Type: application/json'
```

```
##Response
#canCollapse
#language-json
{
"played_at": "2025-08-29T00:00:27.043Z",  
"context": {
	"type": "playlist",
	"external_urls": {
	  "spotify": "https://open.spotify.com/playlist/4gowSSvFaSz8Auzizrjkyw"
	},
	"href": "https://api.spotify.com/v1/playlists/4gowSSvFaSz8Auzizrjkyw",
	"uri": "spotify:playlist:4gowSSvFaSz8Auzizrjkyw"
},
"track": {		
	"album": {
	  "album_type": "album",
	  "artists": [
		{
		  "external_urls": {
			"spotify": "https://open.spotify.com/artist/378dH6EszOLFShpRzAQkVM"
		  },
		  "href": "https://api.spotify.com/v1/artists/378dH6EszOLFShpRzAQkVM",
		  "id": "378dH6EszOLFShpRzAQkVM",
		  "name": "Lindsey Stirling",
		  "type": "artist",
		  "uri": "spotify:artist:378dH6EszOLFShpRzAQkVM"
		}
	  ],
	  "available_markets": [ "AR", "AU", "..." ],
	  "external_urls": {
		"spotify": "https://open.spotify.com/album/5EH0A5mhsGNCOPPpvi3RfF"
	  },
	  "href": "https://api.spotify.com/v1/albums/5EH0A5mhsGNCOPPpvi3RfF",
	  "id": "5EH0A5mhsGNCOPPpvi3RfF",
	  "images": [
		{
		  "url": "https://i.scdn.co/image/ab67616d0000b273af2acf076c7856bfe6ef580e",
		  "width": 640,
		  "height": 640
		},
		{
		  "url": "https://i.scdn.co/image/ab67616d00001e02af2acf076c7856bfe6ef580e",
		  "width": 300,
		  "height": 300
		},
		{
		  "url": "https://i.scdn.co/image/ab67616d00004851af2acf076c7856bfe6ef580e",
		  "width": 64,
		  "height": 64
		}
	  ],
	  "name": "Brave Enough",
	  "release_date": "2016-08-19",
	  "release_date_precision": "day",
	  "total_tracks": 14,
	  "type": "album",
	  "uri": "spotify:album:5EH0A5mhsGNCOPPpvi3RfF"
	},
	"artists": [
	  {
		"external_urls": {
		  "spotify": "https://open.spotify.com/artist/378dH6EszOLFShpRzAQkVM"
		},
		"href": "https://api.spotify.com/v1/artists/378dH6EszOLFShpRzAQkVM",
		"id": "378dH6EszOLFShpRzAQkVM",
		"name": "Lindsey Stirling",
		"type": "artist",
		"uri": "spotify:artist:378dH6EszOLFShpRzAQkVM"
	  },
	  {
		"external_urls": {
		  "spotify": "https://open.spotify.com/artist/31rVRoX5ZG9ZyRbHvlEwjA"
		},
		"href": "https://api.spotify.com/v1/artists/31rVRoX5ZG9ZyRbHvlEwjA",
		"id": "31rVRoX5ZG9ZyRbHvlEwjA",
		"name": "RuthAnne",
		"type": "artist",
		"uri": "spotify:artist:31rVRoX5ZG9ZyRbHvlEwjA"
	  }
	],
	"available_markets": [ "AR", "AU", "..." ],
	"disc_number": 1,
	"duration_ms": 229120,
	"explicit": false,
	"external_ids": {
	  "isrc": "TCACO1625907"
	},
	"external_urls": {
	  "spotify": "https://open.spotify.com/track/0uFwdpkVJzpyUxlu9reTWK"
	},
	"href": "https://api.spotify.com/v1/tracks/0uFwdpkVJzpyUxlu9reTWK",
	"id": "0uFwdpkVJzpyUxlu9reTWK",
	"is_local": false,
	"name": "Love's Just a Feeling (feat. Rooty)",
	"popularity": 39,
	"preview_url": null,
	"track_number": 12,
	"type": "track",
	"uri": "spotify:track:0uFwdpkVJzpyUxlu9reTWK"
  }
}
```

## Step 2: Initial Database Setup

After sampling the data I jumped into the design of the database schema. Keeping it simple initially, I started with a unique table for tracks, artists, albums, and play history, stripping away the fields I didn't want to use.

<div className="flex flex-col lg:flex-row gap-5">  

  <img src="/images/projects/spotify/sptrack-block-first.svg" width="250" alt="Table Diagram - sptrack" />
  <img src="/images/projects/spotify/spartist-block-first.svg" width="250" alt="Table Diagram - spartist" />
  <img src="/images/projects/spotify/spalbum-block-first.svg" width="250" alt="Table Diagram - spalbum" />
  <img src="/images/projects/spotify/spplayhistory-block-end.svg" width="250" alt="Table Diagram - spplayhistory" />

</div>

<div data-component="CallOut" data-header="Important Detail">
  Spotify Play History API is limited to the last 50 tracks played.

  On request Spotify will send you all of "your data" with a notice that it may take up to 30 days to process.
</div>

I submitted a request for my data from Spotify and after a few days I received an email with a link to download my data. The download included 4 JSON files titled like containing my entire Spotify listening history. 
  - The files were ~47MB and contained just over 60,000 records.
  - The format was different than data returned from the Spotify API. It was missing a lot of details.

```
##Bulk Play History Sample
#canCollapse
{
  "ts": "2017-07-27T00:56:06Z",
  "platform": "Android OS 7.0 API 24 (LGE, LGLS992)",
  "ms_played": 75338,
  "conn_country": "US",
  "ip_addr": "73.181.8.221",
  "master_metadata_track_name": "Love's Just a Feeling (feat. Rooty)",
  "master_metadata_album_artist_name": "Lindsey Stirling",
  "master_metadata_album_album_name": "Brave Enough",
  "spotify_track_uri": "spotify:track:0uFwdpkVJzpyUxlu9reTWK",
  "episode_name": null,
  "episode_show_name": null,
  "spotify_episode_uri": null,
  "audiobook_title": null,
  "audiobook_uri": null,
  "audiobook_chapter_uri": null,
  "audiobook_chapter_title": null,
  "reason_start": "clickrow",
  "reason_end": "endplay",
  "shuffle": false,
  "skipped": false,
  "offline": false,
  "offline_timestamp": null,
  "incognito_mode": false
}
```

## Step 3: Historical Data Management 

The next series of steps involved running a handful of one off node and bash scripts to a) transform the historical data, b) enrich the data, and  c) manually load it into the database.

### Step 3a: Transforming the Data

For the transformation, I used array destructuring `items.map(item => {})` and the `...rest` operator to remove unnecessary key value pairs from the bulk data set. I transformed the data to match what was returned from the `api.spotify.com/v1/me/player/recently-played` endpoint and dumped the results out to a new JSON file, `ProcessedSpotifyHistory.json`.

<div data-component="CallOut" data-header="Improvements">

The functions `removeUnwantendPropertiesFromHistory()` and `transformHistoryData(history)` in the code snippet below could be wrapped into one function. I was in a flow state and knew this would only need to run once, so I just kept flowing. 

</div>

```
## Data Transformation Script
#canCollapse
// Data files provided by Spotify
import History1 from './History1.json' with { type: 'json' }; 
import History2 from './History2.json' with { type: 'json' };
import History3 from './History3.json' with { type: 'json' };
import History4 from './History4.json' with { type: 'json' };

import fs from 'fs';
import path from 'path';

function removeUnwantedPropertiesFromHistory(history) {
  return history.map(item => {
    const { 
      platform,
      ms_played,
      conn_country,
      ip_addr,
      master_metadata_album_artist_name,
      episode_name, episode_show_name,
      spotify_episode_uri,
      shuffle,
      audiobook_title,
      audiobook_uri,
      audiobook_chapter_uri,
      audiobook_chapter_title,
      reason_start, reason_end,
      skipped,
      offline,
      offline_timestamp,
      incognito_mode,
      ...rest } = item;
      return rest;
  });
}

function transformHistoryData(history) {
  return history.map(item => {
    item.played_at = item.ts;
    item.album = { name: item.master_metadata_album_album_name || 'Unknown Album' };
    item.title = item.master_metadata_track_name;
    item.songUrl = `https://open.spotify.com/track/${item.spotify_track_uri?.split(':').pop()}`;
    delete item.ts;
    delete item.master_metadata_album_album_name;
    delete item.master_metadata_track_name;
    delete item.spotify_track_uri;
    return item;
  });
}

async function processAndSaveHistory() {
  const allHistory = [...History1, ...History2, ...History3, ...History4];
  const cleanedHistory = removeUnwantedPropertiesFromHistory(allHistory);
  const transformedHistory = transformHistoryData(cleanedHistory);

  if (transformedHistory.length === 0) {
    console.warn('⚠️ No records to write. Check input files.');
    return;
  }

  const directory = process.cwd();
  const outputPath = path.join(directory, 'ProcessedSpotifyHistory.json');
  fs.writeFileSync(outputPath, JSON.stringify(transformedHistory, null, 2));
  console.log(`Processed history saved to ${outputPath}`);
}

processAndSaveHistory().catch(console.error);
```

```
## Bulk Play History Transformed - ProcessedSpotifyHistory.json
#canCollapse
#language-json
{
  "id": "0uFwdpkVJzpyUxlu9reTWK",
  "played_at": "2017-07-27T00:57:32Z",
  "album": {
    "name": "Brave Enough"
  },
  "title": "Love's Just a Feeling (feat. Rooty)",
  "songUrl": "https://open.spotify.com/track/0uFwdpkVJzpyUxlu9reTWK"
}
```

<div data-component="CallOut" data-header="Important Observations">

- A single track can only be on a single album. However, a single track or a single album can have multiple artists.
- The bulk data did not include an id for the track played, however, I extracted it from the `spotify_track_uri` key.
- The track_id was also used to compose the value for the `"songUrl"` key. 

</div>

Before I could load the data into the database I had to acquire the missing track, album, and artist details.

Fortunately Spotify has the `api.spotify.com/v1/artists` and `api.spotify.com/v1/tracks` endpoints for bulk data retriaval. **YAY!**

Unfortunately, you can only request 50 artist or track details at a time. **UGGH!**

### Step 3b - Extract Track Id's
- Extract each track id from the end of every songUrl in `ProcessedSpotifyHistory.json`.
- Write every set of 50 track id's to a file as a JSON array, `SpotifyTrackIdList${count}.json`.

```
## Extraction Script
#canCollapse
import fs from 'fs';
import path from 'path';

var startTime = new Date();

function getTrackIdListFromHistory(history: any[]) { 
  const trackIdSet = new Set();
  const directory = process.cwd();
  var chunkCount = 1;
  
  while (history.length > 0) {
    const chunk = history.splice(0, 50); // Take the first 50 records
    chunk.forEach(item => {
      if (item.songUrl) {
        const trackId = item.songUrl.split('/').pop();
        if (trackId) {
          trackIdSet.add(trackId);
        }
      }
    });

    const outputPath = path.join(directory, `tools/trackHistoryList/spotifyTrackIdList${chunkCount}.json`);
    fs.writeFileSync(outputPath, JSON.stringify(Array.from(trackIdSet), null, 2));
    log(`Track ID list #${chunkCount} saved to ${outputPath}`);
    
    trackIdSet.clear();
    chunkCount++;
  }
  return Array.from(trackIdSet);
} 
```

### Step 3c - Data Enrichment
- Open each `spotifyTrackIdList${count}.json` file. 
- Call `api.spotify.com/v1/tracks?${trackIdList}` and acquire the additional details for every track.
- Write JSON data to file 

```
## Data Enrichment Script
#canCollapse
import fs from 'fs';
import path from 'path';

async function getTrackDetailFromSpotifyTrackIdList(chunkNumber: number, access_token: any) { // STEP 2: Fetch track details from Spotify API
  const inputPath = path.join(directory, `tools/trackHistoryList/spotifyTrackIdList${chunkNumber}.json`);
  
  if (!fs.existsSync(inputPath)) { log(`Input file not found: ${inputPath}`); return; }

  const trackIdList = JSON.parse(fs.readFileSync(inputPath, 'utf-8'));
  if (trackIdList.length === 0) {
    log('⚠️ No track IDs found in the input file.');
    return;
  }

  const chunkSize = 50; // Spotify API allows up to 50 IDs per request
  const trackDetails: any[] = [];

  for (let i = 0; i < trackIdList.length; i += chunkSize) {
    const chunk = trackIdList.slice(i, i + chunkSize);
    const idsParam = chunk.join(','); 

    try {
      const response = await fetch(`https://api.spotify.com/v1/tracks?ids=${idsParam}`, { headers: { Authorization: `Bearer ${access_token}` } });
      
      if (!response.ok) {
        var responseJson = await response.json();
        log(`Error fetching track details: ${response.status} ${response.statusText}`);
        log(`Response: ${responseJson}`);
        continue;
      }
      
      const data = await response.json();
      trackDetails.push(...data.tracks);
    } catch (error) { log(`Fetch error:, ${JSON.stringify(error)}`); }
  }

  const outputPath = path.join(directory, `tools/trackHistoryData/spotifyTrackDetails${chunkNumber}.json`);
  fs.writeFileSync(outputPath, JSON.stringify(trackDetails, null, 2));
  log(`Track details chunk #${chunkNumber} saved to ${outputPath}`);
}

async function getTrackDetailsInChunks() {
 for (let chunkNumber = 1; chunkNumber <= 1224; chunkNumber++) {       
   getTrackDetailFromSpotifyTrackIdList(chunkNumber, access_token);              
   var endtime = new Date();
   if ((endtime.getMilliseconds() - startTime.getMilliseconds()) > 3300000) access_token = getAccessToken();         
   await new Promise(resolve => setTimeout(resolve, 10000));
 }
}

async function joinFiles() {
  var allTrackDetails = [];
  for (let i = 1; i <= 1224; i++) {
    var inputPath = path.join(process.cwd(), `tools/trackHistoryData/spotifyTrackDetails${i}.json`);
    if(!fs.existsSync(inputPath)) { console.log(`File ${inputPath} does not exist, skipping...`); continue; }
    allTrackDetails = allTrackDetails.concat(JSON.parse(fs.readFileSync(inputPath, 'utf-8')));
    console.log(`File #${i} processed, total tracks so far: ${allTrackDetails.length}`);
  }

  var outputPath = path.join(process.cwd(), `tools/trackHistoryData/finalSpotifyTrackDetails.json`);
  fs.writeFileSync(outputPath, JSON.stringify(allTrackDetails, null, 2));
  console.log(`All files joined. Total tracks: ${allTrackDetails.length}`);
}

(async () => {
  await getTrackDetailsInChunks();
  await joinFiles();
  log('All chunks processed and files joined.');
})();
```

### Step 3d - Bash Data Merge
- Using the command line tool, jq, I added details to the `ProcessedSpotifyHistory.json` for every played song.
- Essentially this adds one more nested level of JSON for the track information with the `played_at` key value at the root.
- `{ "played_at": "2017-07-27T00:56:06Z", "track": {...} }, { "played_at": "2017-07-27T00:57:32Z", "track": {...} }`
- This duplicated a lot of data, but when I add it to the database I can use the same structures for future calls to the Spotify API. 
 
```
## Merge Track Details and Play History with jq
#canCollapse
jq -s '
  (.[1] | map({ (.id): . }) | add) as $trackDetails
  |
  .[0] | map(
    (
      .track_id = (.songUrl | split("/track/")[1] | split("?")[0]) |
      .detail = $trackDetails[.track_id] |
      if .detail != null then
        {
          "played_at": .played_at,
          "track": {
            "id": .track_id,
            "name": .detail.name,
            "external_ids": { "isrc": .detail.external_ids.isrc },
            "duration_ms": .detail.duration_ms,
            "explicit": .detail.explicit,
            "external_urls": {
              "spotify": .detail.external_urls.spotify
            },
            "album": {
              "id": .detail.album.id,
              "name": .detail.album.name,
              "release_date": .detail.album.release_date,
              "images": (.detail.album.images // null),
              "artists": (
                .detail.album.artists // [] | map({
                  "id": .id,
                  "name": .name,
                  "external_urls": {
                    "spotify": .external_urls.spotify
                  }
                })
              )
            },
            "artists": (
              .detail.artists | map({
                "id": .id,
                "name": .name,
                "external_urls": {
                  "spotify": .external_urls.spotify
                }
              })
            )
          }
        }
      else
        empty
      end
    )
  )
' "ProcessedSpotifyHistory.json" "finalSpotifyTrackDetails.json" > "merged_spotifyTrackDetails.json"

echo "✅ Merged chunk"
```

### Step 3e - Push the data to the database

- This was the step that made me rethink everything. 
- What you've read so far was likely modified by what occured in this moment.

```
## Database Push
#canCollapse
async function pushTrackDetailsToDB() { 
  var inputPath = path.join(directory, `tools/trackHistoryData/merged_spotifyTrackDetails.json`);
  let trackDetails = await JSON.parse(fs.readFileSync(inputPath, 'utf-8'));

  const CHUNK_SIZE = 400;
  let cursor = 0;
  let chunkNumber = 1;

  while (cursor < trackDetails.length) {
    const dataChunk = trackDetails.slice(cursor, cursor + CHUNK_SIZE);

    log(`🚀 Ingesting chunk #${chunkNumber} with ${dataChunk.length} tracks @ ${new Date().toISOString()}`);

    try {await ingestSpotifyPlays(true, {status: 200, data: dataChunk }); }
    catch (error) { log(`❌ Ingestion error in file chunk #${chunkNumber}: ${JSON.stringify(error)}`); }

    cursor += CHUNK_SIZE;
    chunkNumber++;
  }
  
  log('All files processed.');
}

(async () => {
  pushTrackDetailsToDB();
})();
```

There is so much more that actually happens here when pushing data to the DB. In this case it's all hidden behind the `ingestSpotifyPlays()` function. I utilize `ingestSpotifyPlays()` for both historical data pushes and for real time listenining updates. We'll get to that function later.

<div data-component="CallOut" data-header="Processing Time">

- Actual processing time to import all data into the DB was ~6 hours.
- Total time was closer to 32 hours; script mistakes, PC going to sleep, deciding half way through to do something entirely different and starting over.

</div> 

## Step 4: Update DB Schema (maybe actually 3)

Now that I had prepared all of my historical data and created a process for ingesting it, I realized that my database needed to be properly prepared for the complexities of music data relationships.

### Step 4a - Account for one to many relationships.

The initial simple schema couldn't handle the reality that:
- **A single track can have multiple artists** (features, collaborations)
- **An album can have multiple artists** (compilations, various artists albums)

This required creating junction tables with appropriate relations to properly normalize the many-to-many relationships.

<div><div data-component="SvgScroll" data-src="/images/projects/spotify/schema-first-relations.svg" data-width="600" data-height="800" data-initial-scale="0.6" /></div>

### Step 4b - Performance and accuracy issues calculating top tracks/artists.

<br />

Now, initially I was just manually calculating the "top tracks" and "top artists" by querying the `spplayhistory` table and grouping by track_id or artist_id, ordering by count, and limiting the results. This worked fine for small datasets, but as my listening history grew, these queries became slow and inefficient.

<br />

My first attempt I decided I would precalculate and store the "top" lists in separate tables that could be updated periodically. This would allow for quick retrieval of top tracks and artists without the need for complex queries on the entire play history. Along came the `spdailytrackstat`, `spdailyartiststat` and `spdailyplaysts` tables. I built out the tables and the relationships to very quickly realize that wasn't going to work either.

<div data-component="CallOut" data-header="Clever Detail">

The `hourly_plays` column in the `spdailyplaystats` table is to track the number of plays per hour for any particular day. It is a simple json array of 24 numbers. I thought it was clever.

</div>

<div><div data-component="SvgScroll" data-src="/images/projects/spotify/schema-daily-stats.svg" data-width="600" data-height="800" data-initial-scale="0.6" /></div>

The `spdailyplaystats` works great for the heatmap visualization, but it didn't help with the top artist or top tracks list. Daily stat summaries was incredibly over simplified and grossly misrepresented the data.
- For example, if I listened to a track 10 times in one day, and that track was by Artist A, and I listened to another track by Artist B 5 times on the same day, the daily stats would show Artist A with 10 plays and Artist B with 5 plays for that day. However, if I listened to multiple tracks by Artist A on that same day, the daily stats would not accurately reflect the total plays for Artist A across all tracks.
- This meant that the daily stats could not be used to accurately calculate the top artists or top tracks over a period of time, as they would not account for multiple tracks by the same artist or multiple plays of the same track on the same day.

This was the magic moment, the moment where all the pieces came together for a performant, scalable solution, with an awesome bonus as well!
a) I ditched the `spdailytrackstat` and `spdailyartiststat` tables entirely.
b) I was scratching my head for a while and then came up the bucket strategy.
c) I created the `yearbucket` and `monthbucket` and `weekbucket`. The idea being that I would pre-aggregate the play counts for tracks and artists into these buckets. If a long time range was requested, I could sum the counts from the relevant buckets instead of querying the entire play history and then drill down to the monthl and weekly buckets as needed.

And that was when the final database schema was born (I mean, there were some tweeaks after this, but this was the big one and the rest was just details)!

<div><div data-component="SvgScroll" data-src="/images/projects/spotify/schema-full.svg" data-width="600" data-height="800" data-initial-scale="0.6" /></div>

## Step 5 - Real Time Data Ingestion

So I left off the last time at the `ingestSpotifyPlays()` function. This function is used to ingest both historical data and real time data. Muuh was discovered as I massaged the existing data, and prepated it for the database. The first item you may have noticed in the basic core database tables was **`isrc`** in the `sptrack` table.

The International Standard Recording Code (ISRC) is a unique identifier for sound recordings and music video recordings. It is used to uniquely identify a specific recording of a song or music video, regardless of the format or medium in which it is released. The ISRC is a 12-character alphanumeric code that is assigned to each recording by the record label or distributor.

For example, "Love's Just a Feeling" by Lindsey Stirling could appear multiple times in my database with different track IDs. "Love's Just a Feeling" might exist as:
- The original album version
- A single release
- Part of a compilation

They're fundamentally the same song, identified by their ISRC (International Standard Recording Code). The Spotfiy API endpoint `api.spotify.com/v1/me/top/tracks` returns the top tracks based on the tracck Ids, not the ISRC. This means that if I listened to the same recorded song but on different albums they would be counted separately in my top tracks list.

Well, I didn't like that, so I had to come up with my own solution....recount the top tracks based on ISRC myself.

This also led to adding a `common_album_id` field and a `assignCommonAlbumUrls()` function. If a single recording is on three albums, which album cover should I show? If multiple artists are on a single track, which artist should I show? The solution was to assign a "common" album and artist based on the most frequently played version of the track. 

### Step 5a: Initial Table Data 

Many of these tables requried initial data to be populated. I created a handful of simple scripts to populate these tables with the necessary data.

```
## Creating and filling all of the bucket tables
#canCollapse
export const updateBucketedStats = async (): Promise<void> => {
  const startDate = new Date('2017-07-23'); // First Sunday before first play
  const endDate = new Date(); // Today

  let current = new Date(startDate);

  while (current <= endDate) {
    const year = current.getUTCFullYear();
    const month = current.getUTCMonth() + 1;

    const monthStart = new Date(Date.UTC(year, month - 1, 1));
    const monthEnd = new Date(Date.UTC(year, month, 0, 23, 59, 59, 999));

    console.log(`📆 Processing ${year}-${month.toString().padStart(2, '0')}`);

    const plays = await prisma.spplayhistory.findMany({
      where: { played_at: { gte: monthStart, lte: monthEnd } },
      include: { track: { include: { track_artists: true } } }
    });

    if (plays.length === 0) {
      console.log(`🟡 No plays found for ${year}-${month}, skipping.`);
      await sleep(10000);
      current.setUTCMonth(current.getUTCMonth() + 1);
      continue;
    }

    const buckets = {
      year: new Map(),
      month: new Map(),
      week: new Map()
    };

    for (const play of plays) {
      const date = new Date(play.played_at);
      const y = date.getUTCFullYear();
      const m = date.getUTCMonth() + 1;
      const w = getISOWeek(date);

      const yearKey = `${y}`;
      const monthKey = `${y}-${m}`;
      const weekKey = `${y}-W${w}`;

      const trackId = play.track_id;
      const artistIds = play.track.track_artists.map(a => a.artist_id);

      for (const scope of ['year', 'month', 'week'] as const) {
        const key = scope === 'year' ? yearKey : scope === 'month' ? monthKey : weekKey;
        const bucket = buckets[scope];
        if (!bucket.has(key)) {
          bucket.set(key, { tracks: new Map(), artists: new Map() });
        }

        const stats = bucket.get(key)!;
        stats.tracks.set(trackId, (stats.tracks.get(trackId) ?? 0) + 1);
        for (const artistId of artistIds) {
          stats.artists.set(artistId, (stats.artists.get(artistId) ?? 0) + 1);
        }
      }
    }

    for (const scope of ['year', 'month', 'week'] as const) {
      const bucketMap = buckets[scope];
      for (const [key, { tracks, artists }] of bucketMap.entries()) {
        const [y, mOrW] = key.includes('W') ? key.split('-W') : key.split('-');
        const yInt = parseInt(y);
        const mInt = scope === 'month' || scope === 'week' ? parseInt(mOrW) : undefined;
        const wInt = scope === 'week' ? parseInt(mOrW) : undefined;

        const yearBucket = await prisma.yearbucket.findFirst({ where: { year: yInt } });
        const monthBucket = scope !== 'year'
          ? await prisma.monthbucket.findFirst({ where: { yearbucketid: yearBucket?.id, month: mInt } })
          : undefined;
        const weekBucket = scope === 'week'
          ? await prisma.weekbucket.findFirst({ where: { monthbucketid: monthBucket?.id, week: wInt } })
          : undefined;

        const trackChunks: Prisma.trackstatCreateManyInput[][] =  chunk(Array.from(tracks.entries() as Iterable<[string, number]>).map(([track_id, count]: [string, number]) => (
          { track_id, count, bucket_scope: scope, yearbucketid: yearBucket?.id, monthbucketid: monthBucket?.id, weekbucketid: weekBucket?.id } )), 200 );

        const artistChunks: Prisma.artiststatCreateManyInput[][] =  chunk(Array.from(artists.entries() as Iterable<[string, number]>).map(([artist_id, count]: [string, number]) => (
            { artist_id, count, bucket_scope: scope, yearbucketid: yearBucket?.id, monthbucketid: monthBucket?.id, weekbucketid: weekBucket?.id } )), 200 );

        for (let i = 0; i < trackChunks.length; i++) {
          await prisma.trackstat.createMany({ data: trackChunks[i], skipDuplicates: true });
          console.log(`✅ ${key} track chunk ${i + 1} of ${trackChunks.length} inserted`);
        }

        for (let i = 0; i < artistChunks.length; i++) {
          await prisma.artiststat.createMany({ data: artistChunks[i], skipDuplicates: true });
          console.log(`✅ ${key} artist chunk ${i + 1} of ${artistChunks.length} inserted`);
        }
      }
    }

    await sleep(10000);
    console.log(`🎧 Found ${plays.length} plays for ${year}-${month}`);
    current.setUTCMonth(current.getUTCMonth() + 1);
  }

  console.log('🎉 Bucketed stats update complete');
};

export const backFillNoPlayDays = async (): Promise<void> => {
  const firstPlay = await prisma.spplayhistory.findFirst({ orderBy: { played_at: 'asc' }, select: { played_at: true } });
  if (!firstPlay) { console.error('No play history found.'); return; }

  const start = new Date(firstPlay.played_at);
  const end = new Date();

  const everyDate: Date[] = [];
  for (let d = new Date(start); d <= end; d.setDate(d.getDate() + 1)) { 
    const normalized = new Date(Date.UTC(d.getUTCFullYear(), d.getUTCMonth(), d.getUTCDate()));
    everyDate.push(normalized);
   }

  for (const date of everyDate) {
    const existing = await prisma.spdailyplaystats.findUnique({ where: { date } });
    if (existing) continue;

    await prisma.spdailyplaystats.create({ data: { date, weekday: date.toLocaleDateString('en-US', { weekday: 'short' }), hourly_plays: Array(24).fill(0) } });

    console.log(`🟦 Backfilled empty stats for ${date.toISOString().split('T')[0]}`);
  } 
}
```

### Step 5b: Historical Ingestion

The historical ingestion was covered in Step 3e above. The `ingestSpotifyPlays()` function is used to ingest both historical data and real time data. It is a whopper of a function that handles:
- Inserting new artists, albums, and tracks while avoiding duplicates.
- Updating existing records with new information if necessary.
- Managing relationships between tracks, artists, and albums.
- Handling edge cases like missing data or inconsistencies in the API response.

It was built to be robust and handle a variety of data issues, which means it does a lot of checks and balances that slow it down. For historical data ingestion, performance was less of a concern since it's a one-time operation.

```
## Ingestion Function
#canCollapse
export const ingestSpotifyPlays = async ( manualIngestion: boolean = false, _manualData: RawRecentlyPlayedResponse = { status: 200, data: [] }): Promise<void> => {
  const response = manualIngestion ? _manualData : await getRecentlyPlayedFromSpotify();

  if (response.status !== 200 || !response.data) {
    console.error('Failed to fetch recently played tracks from Spotify');
    console.error(`Error: Received status ${response.status} with no data.`);
    return;
  }

  const items = response.data;
  const allArtistIds = new Set<string>(); // New After first run

  const albumsToInsert: { album_id: string; name: string; image_url: string | null; release_date: Date }[] = [];
  const artistsToInsert: { artist_id: string; name: string; artist_url: string; }[] = [];
  const albumArtistJoins: { album_id: string; artist_id: string; }[] = [];
  const tracksToUpsert: { track_id: string; title: string; isrc: string | null; album_id: string; explicit: boolean; song_url: string | null; duration: number; release_date: Date; }[] = [];
  const trackArtistJoins: { track_id: string; artist_id: string; }[] = [];
  const playHistoryToInsert: { track_id: string; played_at: Date; }[] = [];

  for (const item of items as any[]) {
    const { track, played_at } = item;
    const isrc = track.external_ids?.isrc;

    albumsToInsert.push({ album_id: track.album.id, name: track.album.name, image_url: track.album.images[0]?.url ?? null, release_date: new Date(track.album.release_date) });    
    track.album.artists.forEach((artist: any) => { artistsToInsert.push({ artist_id: artist.id, name: artist.name, artist_url: artist.external_urls.spotify }); albumArtistJoins.push({ album_id: track.album.id, artist_id: artist.id }); });
    tracksToUpsert.push({ track_id: track.id, title: track.name, isrc, album_id: track.album.id, explicit: track.explicit, song_url: track.external_urls.spotify, duration: Math.floor(track.duration_ms / 1000), release_date: new Date(track.album.release_date) });
    track.artists.forEach((artist: any) => { artistsToInsert.push({ artist_id: artist.id, name: artist.name, artist_url: artist.external_urls.spotify }); trackArtistJoins.push({ track_id: track.id, artist_id: artist.id }); });
    playHistoryToInsert.push({ track_id: track.id, played_at: new Date(played_at) });    
  }

  // Deduplicate
  const uniqueAlbums = dedupeById(albumsToInsert, 'album_id');
  const uniqueArtists = dedupeById(artistsToInsert, 'artist_id');
  const uniqueAlbumJoins = dedupeByComposite(albumArtistJoins, ['album_id', 'artist_id']);
  const uniqueTrackJoins = dedupeByComposite(trackArtistJoins, ['track_id', 'artist_id']);

  // Bulk insert
  await prisma.spalbum.createMany({ data: uniqueAlbums, skipDuplicates: true });
  await prisma.spartist.createMany({ data: uniqueArtists, skipDuplicates: true });
  await prisma.spalbumartist.createMany({ data: uniqueAlbumJoins, skipDuplicates: true });
  await prisma.spplayhistory.createMany({ data: playHistoryToInsert, skipDuplicates: true });
  await prisma.sptrack.createMany({ data: tracksToUpsert.map(({ isrc, ...rest }) => ({ ...rest, isrc: isrc ?? '' })), skipDuplicates: true });

  // Upsert tracks and joins
  for (const join of trackArtistJoins) { await prisma.sptrackartist.upsert({ where: { track_id_artist_id: { track_id: join.track_id, artist_id: join.artist_id } }, update: {}, create: join }); }
  for (const join of uniqueTrackJoins) { await prisma.sptrackartist.upsert({ where: { track_id_artist_id: join }, update: {}, create: join }); }

  console.log(`Ingested ${items.length} plays`);

  if (!manualIngestion) {
    const artistsToUpdate = await prisma.spartist.findMany({ where: { image_url: null }, select: { artist_id: true } });
    const idsToUpdate = new Set<string>(artistsToUpdate.map(a => a.artist_id));

    const enrichedArtists = await getArtistsByIds(Array.from(idsToUpdate));
    for (const artist of enrichedArtists) {
      const image = artist.images?.find((image: { width: number }) => image.width === 160);
      await prisma.spartist.update({ where: { artist_id: artist.id }, data: { image_url: image?.url || null }});
    }
    console.log(`🎨 Enriched ${enrichedArtists.length} artists with missing image_url`);
    await assignCommonAlbumUrls();
  } 
};
```

### Step 5c: Real Time Ingestion

All of this work and planning was done to create a dynamic and fast heatmap visualization of my listening history. However, doing many of these calculations client side would make for a very slow and clunky user experience. I needed to have background data ingestion services running in the background.

This is a moment where I have to give credit to the folks over at Render.com, where my profile site is curerntly deployed. NextJS itself is not designed to run background tasks but it is built upon NodeJS...which can run background tasks. Deploying a NextJS site that also exposes a NodeJS server requires a bit of extra configuration.

Admittidly, Render.com had a slew of options for running background workers or cron jobs. But I was stubborn and knew that I had everthhing I needed to run a simple NodeJS script on a schedule. That is why I have a lot of respect for Render and their service, they make annoying or challening things easy but don't inhibit you from doing things your own way also.

``

Render.com allows you to run background workers that can run NodeJS scripts on a schedule. This is perfect for my needs.

```

## Step 6 - Render the Heatmap(s)
